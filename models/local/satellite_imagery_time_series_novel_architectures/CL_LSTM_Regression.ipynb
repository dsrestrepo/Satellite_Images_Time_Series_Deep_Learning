{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf7bcebd",
   "metadata": {},
   "source": [
    "# Setup enviorment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e4738a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/datascience/conda/generalml_p37_gpu_v1/lib/python3.7/site-packages/tensorflow_addons/utils/ensure_tf_install.py:67: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.7.0 and strictly below 2.10.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.6.2 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "# Data reading in Dataframe format and data preprocessing\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Linear algebra operations\n",
    "import numpy as np\n",
    "\n",
    "# Image processing\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "\n",
    "# Machine learning models and preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import Sequential, layers, callbacks\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional, Flatten\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Epiweek\n",
    "from epiweeks import Week, Year\n",
    "\n",
    "# Date\n",
    "from datetime import date as convert_to_date\n",
    "\n",
    "# Os\n",
    "import os\n",
    "\n",
    "# Feature Extraction Model:\n",
    "from Contrastive_Learning_Architecture import get_ContrastiveLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb63c96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd80e407",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = 'DATASET_5_best_cities/'\n",
    "labels = 'Tabular_data/dengue_tabular.csv'\n",
    "MUNICIPALITY = 'Medellín'\n",
    "\n",
    "target_size = (224, 224, 12)\n",
    "\n",
    "backbone = 'Models/contrastive_learning_encoder_with_projection_head.h5'\n",
    "\n",
    "cities =  {\n",
    "  \"76001\": \"Cali\",\n",
    "  \"05001\": \"Medellín\",\n",
    "  \"50001\": \"Villavicencio\",\n",
    "  \"54001\": \"Cúcuta\",\n",
    "  \"73001\": \"Ibagué\",\n",
    "  \"68001\": \"Bucaramanga\",\n",
    "  \"05360\": \"Itagüí\",\n",
    "  \"08001\": \"Barranquilla\",\n",
    "  \"41001\": \"Neiva\",\n",
    "  \"23001\": \"Montería\"\n",
    "}\n",
    "\n",
    "codes =  {\n",
    "  \"Cali\": \"76001\",\n",
    "  \"Medellín\": \"05001\",\n",
    "  \"Villavicencio\": \"50001\",\n",
    "  \"Cúcuta\": \"54001\",\n",
    "  \"Ibagué\": \"73001\",\n",
    "  \"Bucaramanga\": \"68001\",\n",
    "  \"Itagüí\": \"05360\",\n",
    "  \"Barranquilla\": \"08001\",\n",
    "  \"Neiva\": \"41001\",\n",
    "  \"Montería\": \"23001\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da45af16",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c33298bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epiweek_from_date(image_date):\n",
    "    date = image_date.split('-')\n",
    "    \n",
    "    # Get year as int\n",
    "    year = ''.join(filter(str.isdigit, date[0]))\n",
    "    year = int(year)\n",
    "    \n",
    "    # Get month as int\n",
    "    month = ''.join(filter(str.isdigit, date[1]))\n",
    "    month = int(month)\n",
    "    \n",
    "    # Get day as int\n",
    "    day = ''.join(filter(str.isdigit, date[2]))\n",
    "    day = int(day)\n",
    "    \n",
    "    # Get epiweek:\n",
    "    date = convert_to_date(year, month, day)\n",
    "    epiweek = str(Week.fromdate(date))\n",
    "    epiweek = int(epiweek)\n",
    "    \n",
    "    return epiweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7af741be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epiweek(name):\n",
    "    \n",
    "    # Get week\n",
    "    week = name.split('/')[1]\n",
    "    week = week.replace('w','')\n",
    "    week = int(week)\n",
    "    \n",
    "    # Year\n",
    "    year = name.split('/')[0]\n",
    "    year = int(year)\n",
    "    \n",
    "    epiweek = Week(year, week)\n",
    "    \n",
    "    epiweek = str(epiweek)\n",
    "    epiweek = int(epiweek)\n",
    "\n",
    "    return epiweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a06b440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_labels(path, Municipality = None):\n",
    "    df = pd.read_csv(path)\n",
    "    if df.shape[1] > 678:\n",
    "        df = pd.concat([df[['Municipality code', 'Municipality']], df.iloc[:,-676:]], axis=1)\n",
    "        cols = df.iloc[:, 2:].columns\n",
    "        new_cols = df.iloc[:, 2:].columns.to_series().apply(get_epiweek)\n",
    "        df = df.rename(columns=dict(zip(cols, new_cols))) \n",
    "        \n",
    "    if 'Label_CSV_All_Municipality' in path:\n",
    "        # Get Columns\n",
    "        df = df[['epiweek', 'Municipality code', 'Municipality', 'final_cases_label']]\n",
    "        \n",
    "        # change epiweek format\n",
    "        df.epiweek = df.epiweek.apply(get_epiweek)\n",
    "        \n",
    "        # Remove duplicates\n",
    "        df = df[df.duplicated(['epiweek','Municipality code','Municipality']) == False]\n",
    "        \n",
    "        # Replace Increase, decrease, stable to numerical:\n",
    "        \"\"\"\n",
    "        - Stable = 0\n",
    "        - Increased = 1 \n",
    "        - Decreased = 2\n",
    "        \"\"\"\n",
    "        df.final_cases_label = df.final_cases_label.replace({'Stable': 0, 'Increased': 1, 'Decreased': 2})\n",
    "        \n",
    "        # Create table\n",
    "        df = df.pivot(index=['Municipality code', 'Municipality'], columns='epiweek', values='final_cases_label')\n",
    "\n",
    "        # Reset Index:\n",
    "        df = df.reset_index()\n",
    "    \n",
    "    if Municipality:\n",
    "        df = df[df['Municipality'] == Municipality]\n",
    "        df.drop(columns=['Municipality code'], inplace=True)\n",
    "        df.rename(columns={'Municipality': 'Municipality Code'}, inplace=True)\n",
    "    \n",
    "        df = df.set_index('Municipality Code')\n",
    "        df = df.T\n",
    "\n",
    "        df.columns.name = None\n",
    "        df.index.name = None\n",
    "        \n",
    "        df.columns = ['Cases']\n",
    "    \n",
    "    #df = df.reset_index()\n",
    "    #df.rename(columns={'index': 'epiweek'}, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a64df28",
   "metadata": {},
   "source": [
    "### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a16338a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200701</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200702</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200703</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200704</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200705</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201948</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201949</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201950</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201951</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201952</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>676 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Cases\n",
       "200701      1\n",
       "200702      0\n",
       "200703      0\n",
       "200704      0\n",
       "200705      0\n",
       "...       ...\n",
       "201948     15\n",
       "201949     20\n",
       "201950     30\n",
       "201951     14\n",
       "201952      5\n",
       "\n",
       "[676 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df = read_labels(path=labels, Municipality=MUNICIPALITY)\n",
    "labels_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44922e89",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ffb23ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(images_dir):\n",
    "    \n",
    "    out_df = {\n",
    "        'epiweek':[],\n",
    "        'image':[]\n",
    "    }\n",
    "    \n",
    "    for image_path in os.listdir(images_dir):\n",
    "        if image_path.endswith('.tiff'):\n",
    "            epiweek = epiweek_from_date(image_path)\n",
    "            full_path = os.path.join(images_dir, image_path)\n",
    "            \n",
    "            out_df['epiweek'].append(epiweek)\n",
    "            out_df['image'].append(full_path)\n",
    "\n",
    "    df = pd.DataFrame(out_df)\n",
    "    \n",
    "    df = df.set_index('epiweek')\n",
    "    df.index.name = None\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9669c0e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>201731</th>\n",
       "      <td>DATASET_5_best_cities/Medellín/image_2017-07-3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201551</th>\n",
       "      <td>DATASET_5_best_cities/Medellín/image_2015-12-2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201747</th>\n",
       "      <td>DATASET_5_best_cities/Medellín/image_2017-11-1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201647</th>\n",
       "      <td>DATASET_5_best_cities/Medellín/image_2016-11-2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201729</th>\n",
       "      <td>DATASET_5_best_cities/Medellín/image_2017-07-1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    image\n",
       "201731  DATASET_5_best_cities/Medellín/image_2017-07-3...\n",
       "201551  DATASET_5_best_cities/Medellín/image_2015-12-2...\n",
       "201747  DATASET_5_best_cities/Medellín/image_2017-11-1...\n",
       "201647  DATASET_5_best_cities/Medellín/image_2016-11-2...\n",
       "201729  DATASET_5_best_cities/Medellín/image_2017-07-1..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if 'DATASET_5_best_cities' in features:\n",
    "    MUNICIPALITY = MUNICIPALITY\n",
    "else:\n",
    "    MUNICIPALITY = codes[MUNICIPALITY]\n",
    "    \n",
    "images_dir = os.path.join(features, MUNICIPALITY)\n",
    "\n",
    "features_df = create_df(images_dir)\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af51ba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(path, target_size):\n",
    "    # Read the image and convert to numpy array\n",
    "    image = io.imread(path)\n",
    "    # Resize the image and normalize values\n",
    "    image_arr = resize(image,(target_size[0], target_size[1]))\n",
    "    #print(f'The shape of the image before reshape: {image_arr.shape}, of type{type(image_arr)}')\n",
    "\n",
    "    # Select RGB bands\n",
    "    if target_size[2] == 3:\n",
    "        image_arr = image_arr[:,:, [1,2,3]]\n",
    "    return image_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85d38a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.image = features_df.image.apply(read_image, args=[target_size])\n",
    "#features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd66b409",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd25ec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = labels_df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2d1bdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two dataframes based on the date values\n",
    "dengue_df = features_df.merge(labels_df, how='inner', left_index=True, right_index=True)\n",
    "dengue_df = dengue_df.sort_index()\n",
    "#dengue_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a458e61",
   "metadata": {},
   "source": [
    "### Train Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74f538cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, train_percentage = 80):\n",
    "    # We need a sequence so we can't split randomly\n",
    "    # To divide into Train and test we have to calculate the train percentage of the dataset:\n",
    "    size = df.shape[0]\n",
    "    split = int(size*(train_percentage/100))\n",
    "    \n",
    "    \"\"\" Train \"\"\"\n",
    "    # We will train with 1st percentage % of data and test with the rest\n",
    "    train_df = df.iloc[:split,:] ## percentage % train\n",
    "    \n",
    "    \"\"\" Test \"\"\"\n",
    "    test_df = df.iloc[split:,:] # 100 - percentage % test\n",
    "    \n",
    "    print(f'The train shape is: {train_df.shape}')\n",
    "    print(f'The test shape is: {test_df.shape}')\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2044438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train shape is: (132, 2)\n",
      "The test shape is: (33, 2)\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(dengue_df, train_percentage = 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7c5d61",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "900e02d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_train_labels(df, column, feature_range=(-1, 1)):\n",
    "    # Get values of the column\n",
    "    values = df[column].values.reshape(-1,1)\n",
    "    # Generate a new scaler\n",
    "    scaler = MinMaxScaler(feature_range=feature_range)\n",
    "    # Fit the scaler just for that column\n",
    "    scaled_column = scaler.fit_transform(values)\n",
    "    # Add the scaled column to the dataframe\n",
    "    scaled_column = np.reshape(scaled_column, len(scaled_column))\n",
    "    df[column] = scaled_column\n",
    "    return df, scaler\n",
    "    \n",
    "def normalize_test_labels(df, column, scaler):\n",
    "    # Get values of the column\n",
    "    values = df[column].values.reshape(-1,1)\n",
    "    # Scale values\n",
    "    scaled_column = scaler.transform(values)\n",
    "    scaled_column = np.reshape(scaled_column,len(scaled_column))\n",
    "    # Add the scaled values to the df\n",
    "    df[column] = scaled_column\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2021686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201544   -0.692042\n",
       "201545   -0.629758\n",
       "201546   -0.685121\n",
       "201547   -0.712803\n",
       "201548   -0.581315\n",
       "Name: Cases, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_range = (-1, 1)\n",
    "\n",
    "# Scale train:\n",
    "train_df, scaler = normalize_train_labels(train_df, 'Cases', feature_range=feature_range)\n",
    "\n",
    "train_df['Cases'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbfb9c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201820   -0.975779\n",
       "201821   -0.989619\n",
       "201822   -0.982699\n",
       "201823   -0.968858\n",
       "201824   -0.972318\n",
       "Name: Cases, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scale test:\n",
    "test_df = normalize_test_labels(test_df, 'Cases', scaler=scaler)\n",
    "test_df['Cases'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83d9620",
   "metadata": {},
   "source": [
    "### Prepare data for time series supervised learning (function to create sliding window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86d5d131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for time series\n",
    "\n",
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True, no_autoregressive=None):\n",
    "    if no_autoregressive:\n",
    "        n_in = n_in - 1\n",
    "        \n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        if no_autoregressive:\n",
    "            cols.append(df.shift(i).iloc[:,:-1])\n",
    "            names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars-1)]\n",
    "        else:\n",
    "            cols.append(df.shift(i))\n",
    "            names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f261613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of window\n",
    "days = 10\n",
    "no_autoregressive = True\n",
    "\n",
    "# frame as supervised learning\n",
    "train = series_to_supervised(train_df, n_in=days, no_autoregressive=no_autoregressive)\n",
    "test = series_to_supervised(test_df, n_in=days, no_autoregressive=no_autoregressive)\n",
    "\n",
    "#DataFrame(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59e523d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_df_to_np(train):\n",
    "    for i, column in enumerate(train.columns):\n",
    "        if i == 0:\n",
    "            train_arr = np.array(train[column].to_list())\n",
    "            train_arr = np.expand_dims(train_arr, axis=1)\n",
    "\n",
    "        else:\n",
    "            #print(f'original: {train_arr.shape}')\n",
    "\n",
    "            train_arr_aux = np.array(train[column].to_list())\n",
    "            train_arr_aux = np.expand_dims(train_arr_aux, axis=1)\n",
    "\n",
    "            #print(f'aux: {train_arr_aux.shape}')\n",
    "\n",
    "            train_arr = np.concatenate((train_arr, train_arr_aux), axis=1)\n",
    "\n",
    "    return train_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cbc708",
   "metadata": {},
   "source": [
    "### Features and Labels Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00542e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_labels_set(timeseries_data, original_df):\n",
    "    \n",
    "    \"\"\" Features \"\"\"\n",
    "    # We define the number of features as (Cases and media cloud)\n",
    "    n_features = original_df.shape[1]\n",
    "\n",
    "    # The features to train the model will be all except the values of the actual week \n",
    "    # We can't use other variables in week t because whe need to resample a a 3D Array\n",
    "    features_set = DataFrame(timeseries_data.values[:,:-1])\n",
    "    # Convert pandas data frame to np.array to reshape as 3D Array\n",
    "    features_set = convert_df_to_np(features_set)\n",
    "    print(f'The shape of the features is {features_set.shape}')\n",
    "    \n",
    "    \"\"\" Labels \"\"\"\n",
    "    # We will use Covid cases in last week \n",
    "    labels_set = DataFrame(timeseries_data.values[:,-1])\n",
    "    # Convert pandas data frame to np.array\n",
    "    labels_set = labels_set.to_numpy()\n",
    "    print(f'The shape of the labels is {labels_set.shape}')\n",
    "    \n",
    "    return features_set, labels_set, n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62f48933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "The shape of the features is (123, 10, 224, 224, 12)\n",
      "The shape of the labels is (123, 1)\n",
      "Test:\n",
      "The shape of the features is (24, 10, 224, 224, 12)\n",
      "The shape of the labels is (24, 1)\n"
     ]
    }
   ],
   "source": [
    "# Train features and labels set\n",
    "print('Train:')\n",
    "train_X, train_y, n_features = features_labels_set(timeseries_data=train, original_df=dengue_df)\n",
    "\n",
    "# Test features and labels set\n",
    "print('Test:')\n",
    "test_X, test_y, n_features = features_labels_set(timeseries_data=test, original_df=dengue_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6164d161",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.asarray(train_X).astype(np.float32)\n",
    "train_y = np.asarray(train_y).astype(np.float32)\n",
    "\n",
    "test_X = np.asarray(test_X).astype(np.float32)\n",
    "test_y = np.asarray(test_y).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a718c757",
   "metadata": {},
   "source": [
    "# Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "300c4db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seed\n",
    "#tf.random.set_seed(0)\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    epsilon = 0.1\n",
    "    summ = K.maximum(K.abs(y_true) + K.abs(y_pred) + epsilon, 0.5 + epsilon)\n",
    "    smape = K.abs(y_pred - y_true) / summ * 2.0\n",
    "    return smape\n",
    "\n",
    "def create_model(backbone=backbone):\n",
    "    lstm_week, input_shape = days, target_size\n",
    "    \n",
    "    # design network\n",
    "    model = Sequential()\n",
    "\n",
    "    # CNN\n",
    "    cnn = get_ContrastiveLearning(model_path=backbone, backbone=True)\n",
    "\n",
    "    for idx, layer in enumerate(cnn.layers):\n",
    "        layer.trainable = False # idx > len(cnn.layers) - 2 \n",
    "    \n",
    "    # https://levelup.gitconnected.com/hands-on-practice-with-time-distributed-layers-using-tensorflow-c776a5d78e7e\n",
    "    model.add(keras.layers.TimeDistributed(cnn, input_shape = ((lstm_week,) + input_shape)))\n",
    "    model.add(keras.layers.TimeDistributed(Flatten()))\n",
    "    model.add(keras.layers.TimeDistributed(Dense(1024)))\n",
    "    model.add(LSTM(120, dropout=0.1, return_sequences=True))\n",
    "    model.add(LSTM(240, dropout=0.1, return_sequences = False))\n",
    "    model.add(Dense(60))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Compile the model:\n",
    "    opt = keras.optimizers.Adam()\n",
    "    \n",
    "    # Metrics\n",
    "    metrics = [\n",
    "        tf.keras.metrics.RootMeanSquaredError(name='rmse'),\n",
    "        tf.keras.metrics.MeanAbsolutePercentageError(name='mape'),\n",
    "        smape\n",
    "    ]\n",
    "    \n",
    "    model.compile(loss='mse', optimizer=opt, metrics=metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94501fff",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87e615a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# EarlyStopping:\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=20, \n",
    "        verbose=1, mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca2505bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit network\n",
    "def train_model(model, monitor, plot=None, epochs=20):\n",
    "    if monitor:\n",
    "        history = model.fit(train_X, train_y, epochs=epochs, batch_size=16, validation_data=(test_X, test_y), verbose=2, shuffle=False, callbacks=[monitor])\n",
    "    else:\n",
    "        history = model.fit(train_X, train_y, epochs=epochs, batch_size=16, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "    \n",
    "    if plot:\n",
    "        # plot history\n",
    "        plt.plot(history.history['loss'], label='train')\n",
    "        plt.plot(history.history['val_loss'], label='validation')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c83db27",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4fe8c8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "\n",
    "def test_model(model, test_X, test_y, scaler, rnn = None):\n",
    "    \n",
    "    # If model is a classical machine learning model and test_X is a 3D tensor, then convert to 2D\n",
    "    if not rnn and (len(test_X.shape) == 3):\n",
    "        test_X = test_X.reshape((test_X.shape[0], -1))\n",
    "    \n",
    "    # do the prediction\n",
    "    yhat = model.predict(test_X)\n",
    "    \n",
    "    # Invert scaling for forecast\n",
    "    # Inverse Scaler\n",
    "    \n",
    "    # Predicted\n",
    "    if not rnn:\n",
    "        yhat = yhat.reshape(-1, 1)\n",
    "        \n",
    "    if not scaler:\n",
    "        return yhat, test_y\n",
    "    \n",
    "    inv_yhat = scaler.inverse_transform(yhat)\n",
    "    \n",
    "    # Real:\n",
    "    inv_y = scaler.inverse_transform(test_y)\n",
    "    \n",
    "    return inv_yhat, inv_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe80dfbc",
   "metadata": {},
   "source": [
    "### Mean Absolute Percentage Error (MAPE)\n",
    "\n",
    "$$\n",
    "MAPE = \\displaystyle\\frac{100\\%}{n}\\sum_{t=1}^{n}\\left |\\frac{x_i-y_i}{y_t}\\right|\n",
    "$$\n",
    "\n",
    "MAPE has a problem if there are zeros in the test data, so other metrics can be explored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e75caeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    print('Test MAPE: %.3f' % mape)\n",
    "    return mape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22d1c62",
   "metadata": {},
   "source": [
    "### Symmetric Mean Absolute Percentage Error (sMAPE)\n",
    "\n",
    "$$\n",
    "sMAPE = \\displaystyle\\frac{100\\%}{n}\\sum_{t=1}^{n} \\frac{|x_i-y_i|}{|x_i|+|y_t|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6849936f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def symmetric_mean_absolute_percentage_error(y_true, y_pred):\n",
    "    \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    smape = 1/len(y_true) * np.sum(2 * np.abs(y_pred-y_true) / (np.abs(y_true) + np.abs(y_pred))*100)\n",
    "    print('Test sMAPE: %.3f' % smape)\n",
    "    return smape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a8623d",
   "metadata": {},
   "source": [
    "### Root Mean Squared Error (RMSE)\n",
    "$$\n",
    "RMSE = \\sqrt{(\\frac{1}{n})\\sum_{i=1}^{n}(x_i-y_i)^{2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ad6f8287",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    print('Test RMSE: %.3f' % rmse)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08e0c9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(inv_y, inv_yhat, model_name = ''):\n",
    "    data_predict = inv_yhat  ## predicted target cases\n",
    "    dataY_plot = inv_y  ##  real test-target cases\n",
    "\n",
    "    data_predict = data_predict.reshape(len(data_predict), 1)\n",
    "    dataY_plot = dataY_plot.reshape(len(dataY_plot), 1)\n",
    "\n",
    "    plt.plot(dataY_plot, label = 'actual')\n",
    "    plt.plot(data_predict, label = 'predicted')\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.suptitle(f'Time-Series Prediction with {model_name}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4398088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_X, test_y, scaler):\n",
    "    stored_results = {}\n",
    "    inv_yhat_lstm, inv_y_lstm = test_model(model=model, test_X=test_X, test_y=test_y, scaler=scaler, rnn = True)\n",
    "    stored_results['mape'] = mean_absolute_percentage_error(inv_y_lstm, inv_yhat_lstm)\n",
    "    stored_results['smape'] = symmetric_mean_absolute_percentage_error(inv_y_lstm, inv_yhat_lstm)\n",
    "    stored_results['rmse'] = root_mean_squared_error(inv_y_lstm, inv_yhat_lstm)\n",
    "\n",
    "    return stored_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "349ebd4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmodel = create_model(backbone=backbone)\\ntrain_model(model=model, monitor=monitor)\\n\\nmodel.save(f'Models/{backbone}_LSTM_Regression.h5')\\nmodel.summary()\\n\\ninv_yhat_lstm, inv_y_lstm = test_model(model=model, test_X=test_X, test_y=test_y, scaler=scaler, rnn = True)\\n\\nevaluate(model, test_X, test_y, scaler)\\n\\n# LSTM\\nplot_predictions(inv_y_lstm, inv_yhat_lstm, model_name = 'LSTM')\\n\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "model = create_model(backbone=backbone)\n",
    "train_model(model=model, monitor=monitor)\n",
    "\n",
    "model.save(f'Models/{backbone}_LSTM_Regression.h5')\n",
    "model.summary()\n",
    "\n",
    "inv_yhat_lstm, inv_y_lstm = test_model(model=model, test_X=test_X, test_y=test_y, scaler=scaler, rnn = True)\n",
    "\n",
    "evaluate(model, test_X, test_y, scaler)\n",
    "\n",
    "# LSTM\n",
    "plot_predictions(inv_y_lstm, inv_yhat_lstm, model_name = 'LSTM')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866094cb",
   "metadata": {},
   "source": [
    "# Calculate Mean and SD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d378925c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With LSTM:\n",
    "#print(f'The scalers are: {scalers.keys()}')\n",
    "#y_scaler = scalers['scaler_Cases']\n",
    "\n",
    "def calculate_mean_std():\n",
    "    \n",
    "    metrics = {\n",
    "        \"rmse\": [],\n",
    "        \"mape\": [],\n",
    "        \"smape\": []\n",
    "    }\n",
    "    \n",
    "    for i in range(5):\n",
    "        model = create_model(backbone=backbone)\n",
    "        train_model(model=model, monitor=monitor)\n",
    "        stored_results = evaluate(model, test_X, test_y, scaler)\n",
    "        print(stored_results)\n",
    "        \n",
    "        for key in metrics.keys():\n",
    "            metrics[key].append(stored_results[key])\n",
    "            \n",
    "    for key in metrics.keys():\n",
    "        results = metrics[key]\n",
    "        print(key, f\": average={np.average(results):.3f}, std={np.std(results):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4a90c3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "8/8 - 17s - loss: 2.0699 - rmse: 1.4387 - mape: 205440.1562 - smape: 1.1452 - val_loss: 0.1338 - val_rmse: 0.3657 - val_mape: 37.5924 - val_smape: 0.3030\n",
      "Epoch 2/20\n",
      "8/8 - 10s - loss: 1.0139 - rmse: 1.0069 - mape: 11188325.0000 - smape: 1.1956 - val_loss: 0.3741 - val_rmse: 0.6116 - val_mape: 62.8287 - val_smape: 0.8523\n",
      "Epoch 3/20\n",
      "8/8 - 10s - loss: 0.3730 - rmse: 0.6107 - mape: 3057613.5000 - smape: 0.9507 - val_loss: 0.1645 - val_rmse: 0.4056 - val_mape: 41.6139 - val_smape: 0.4936\n",
      "Epoch 4/20\n",
      "8/8 - 10s - loss: 0.3822 - rmse: 0.6183 - mape: 4364911.5000 - smape: 0.8104 - val_loss: 0.1240 - val_rmse: 0.3521 - val_mape: 36.1095 - val_smape: 0.4148\n",
      "Epoch 5/20\n",
      "8/8 - 10s - loss: 0.4282 - rmse: 0.6544 - mape: 4997527.5000 - smape: 0.8910 - val_loss: 0.1967 - val_rmse: 0.4436 - val_mape: 45.5285 - val_smape: 0.5528\n",
      "Epoch 6/20\n",
      "8/8 - 10s - loss: 0.3910 - rmse: 0.6253 - mape: 4186228.7500 - smape: 0.8982 - val_loss: 0.2050 - val_rmse: 0.4528 - val_mape: 46.4803 - val_smape: 0.5677\n",
      "Epoch 7/20\n",
      "8/8 - 10s - loss: 0.3783 - rmse: 0.6151 - mape: 3921287.5000 - smape: 0.8740 - val_loss: 0.1799 - val_rmse: 0.4242 - val_mape: 43.5388 - val_smape: 0.5224\n",
      "Epoch 8/20\n",
      "8/8 - 10s - loss: 0.3801 - rmse: 0.6165 - mape: 4154754.7500 - smape: 0.8583 - val_loss: 0.1797 - val_rmse: 0.4239 - val_mape: 43.5092 - val_smape: 0.5219\n",
      "Epoch 9/20\n",
      "8/8 - 10s - loss: 0.3922 - rmse: 0.6262 - mape: 4536087.5000 - smape: 0.8782 - val_loss: 0.1880 - val_rmse: 0.4336 - val_mape: 44.5054 - val_smape: 0.5371\n",
      "Epoch 10/20\n",
      "8/8 - 10s - loss: 0.3770 - rmse: 0.6140 - mape: 4491597.5000 - smape: 0.8697 - val_loss: 0.1852 - val_rmse: 0.4304 - val_mape: 44.1760 - val_smape: 0.5320\n",
      "Epoch 11/20\n",
      "8/8 - 10s - loss: 0.3788 - rmse: 0.6155 - mape: 4464337.5000 - smape: 0.8626 - val_loss: 0.1854 - val_rmse: 0.4306 - val_mape: 44.2032 - val_smape: 0.5325\n",
      "Epoch 12/20\n",
      "8/8 - 10s - loss: 0.3849 - rmse: 0.6204 - mape: 4588597.0000 - smape: 0.8700 - val_loss: 0.1898 - val_rmse: 0.4357 - val_mape: 44.7262 - val_smape: 0.5405\n",
      "Epoch 13/20\n",
      "8/8 - 10s - loss: 0.3827 - rmse: 0.6186 - mape: 4327588.5000 - smape: 0.8736 - val_loss: 0.1935 - val_rmse: 0.4399 - val_mape: 45.1536 - val_smape: 0.5470\n",
      "Epoch 14/20\n",
      "8/8 - 10s - loss: 0.3785 - rmse: 0.6153 - mape: 4559527.5000 - smape: 0.8702 - val_loss: 0.1908 - val_rmse: 0.4368 - val_mape: 44.8402 - val_smape: 0.5422\n",
      "Epoch 15/20\n",
      "8/8 - 10s - loss: 0.3750 - rmse: 0.6124 - mape: 4934832.5000 - smape: 0.8616 - val_loss: 0.1869 - val_rmse: 0.4323 - val_mape: 44.3697 - val_smape: 0.5350\n",
      "Epoch 16/20\n",
      "8/8 - 10s - loss: 0.3784 - rmse: 0.6151 - mape: 4428763.5000 - smape: 0.8655 - val_loss: 0.1894 - val_rmse: 0.4352 - val_mape: 44.6743 - val_smape: 0.5397\n",
      "Epoch 17/20\n",
      "8/8 - 10s - loss: 0.3725 - rmse: 0.6103 - mape: 4479184.0000 - smape: 0.8627 - val_loss: 0.1896 - val_rmse: 0.4355 - val_mape: 44.6991 - val_smape: 0.5401\n",
      "Epoch 18/20\n",
      "8/8 - 10s - loss: 0.3811 - rmse: 0.6173 - mape: 4405675.0000 - smape: 0.8631 - val_loss: 0.1960 - val_rmse: 0.4427 - val_mape: 45.4449 - val_smape: 0.5515\n",
      "Epoch 19/20\n",
      "8/8 - 10s - loss: 0.3789 - rmse: 0.6156 - mape: 4107663.7500 - smape: 0.8715 - val_loss: 0.1954 - val_rmse: 0.4420 - val_mape: 45.3694 - val_smape: 0.5504\n",
      "Epoch 20/20\n",
      "8/8 - 10s - loss: 0.3737 - rmse: 0.6113 - mape: 4531456.0000 - smape: 0.8657 - val_loss: 0.1938 - val_rmse: 0.4403 - val_mape: 45.1914 - val_smape: 0.5476\n",
      "Test MAPE: 631.540\n",
      "Test sMAPE: 149.122\n",
      "Test RMSE: 127.236\n",
      "{'mape': 631.5395832061768, 'smape': 149.12195841471353, 'rmse': 127.2361}\n",
      "Epoch 1/20\n",
      "8/8 - 17s - loss: 1.7086 - rmse: 1.3071 - mape: 3516779.7500 - smape: 1.2138 - val_loss: 0.2770 - val_rmse: 0.5263 - val_mape: 54.0270 - val_smape: 0.4082\n",
      "Epoch 2/20\n",
      "8/8 - 10s - loss: 1.1642 - rmse: 1.0790 - mape: 12324483.0000 - smape: 1.5010 - val_loss: 0.4360 - val_rmse: 0.6603 - val_mape: 67.8485 - val_smape: 0.9528\n",
      "Epoch 3/20\n",
      "8/8 - 10s - loss: 0.3010 - rmse: 0.5486 - mape: 2696904.5000 - smape: 0.7877 - val_loss: 0.0528 - val_rmse: 0.2298 - val_mape: 23.5024 - val_smape: 0.2518\n",
      "Epoch 4/20\n",
      "8/8 - 10s - loss: 0.4887 - rmse: 0.6991 - mape: 6251353.0000 - smape: 0.8512 - val_loss: 0.1776 - val_rmse: 0.4214 - val_mape: 43.2536 - val_smape: 0.5181\n",
      "Epoch 5/20\n",
      "8/8 - 10s - loss: 0.4303 - rmse: 0.6560 - mape: 4546823.0000 - smape: 0.9640 - val_loss: 0.2483 - val_rmse: 0.4983 - val_mape: 51.1662 - val_smape: 0.6433\n",
      "Epoch 6/20\n",
      "8/8 - 10s - loss: 0.3740 - rmse: 0.6115 - mape: 3881544.5000 - smape: 0.8916 - val_loss: 0.1686 - val_rmse: 0.4106 - val_mape: 42.1345 - val_smape: 0.5013\n",
      "Epoch 7/20\n",
      "8/8 - 10s - loss: 0.3921 - rmse: 0.6262 - mape: 4695690.0000 - smape: 0.8517 - val_loss: 0.1588 - val_rmse: 0.3985 - val_mape: 40.8902 - val_smape: 0.4829\n",
      "Epoch 8/20\n",
      "8/8 - 10s - loss: 0.3991 - rmse: 0.6317 - mape: 4579563.5000 - smape: 0.8743 - val_loss: 0.1851 - val_rmse: 0.4302 - val_mape: 44.1561 - val_smape: 0.5318\n",
      "Epoch 9/20\n",
      "8/8 - 10s - loss: 0.3897 - rmse: 0.6242 - mape: 4498990.0000 - smape: 0.8808 - val_loss: 0.1921 - val_rmse: 0.4382 - val_mape: 44.9836 - val_smape: 0.5444\n",
      "Epoch 10/20\n",
      "8/8 - 10s - loss: 0.3849 - rmse: 0.6204 - mape: 4482474.0000 - smape: 0.8736 - val_loss: 0.1787 - val_rmse: 0.4228 - val_mape: 43.3884 - val_smape: 0.5201\n",
      "Epoch 11/20\n",
      "8/8 - 10s - loss: 0.3929 - rmse: 0.6269 - mape: 4355282.0000 - smape: 0.8700 - val_loss: 0.1810 - val_rmse: 0.4254 - val_mape: 43.6599 - val_smape: 0.5243\n",
      "Epoch 12/20\n",
      "8/8 - 10s - loss: 0.3821 - rmse: 0.6182 - mape: 4783386.0000 - smape: 0.8700 - val_loss: 0.1845 - val_rmse: 0.4295 - val_mape: 44.0759 - val_smape: 0.5306\n",
      "Epoch 13/20\n",
      "8/8 - 10s - loss: 0.3788 - rmse: 0.6155 - mape: 4598281.5000 - smape: 0.8551 - val_loss: 0.1852 - val_rmse: 0.4303 - val_mape: 44.1602 - val_smape: 0.5319\n",
      "Epoch 14/20\n",
      "8/8 - 10s - loss: 0.3804 - rmse: 0.6167 - mape: 4573896.0000 - smape: 0.8727 - val_loss: 0.1806 - val_rmse: 0.4250 - val_mape: 43.6055 - val_smape: 0.5235\n",
      "Epoch 15/20\n",
      "8/8 - 10s - loss: 0.3760 - rmse: 0.6132 - mape: 4695131.5000 - smape: 0.8570 - val_loss: 0.1847 - val_rmse: 0.4298 - val_mape: 44.0894 - val_smape: 0.5309\n",
      "Epoch 16/20\n",
      "8/8 - 10s - loss: 0.3711 - rmse: 0.6092 - mape: 4274653.5000 - smape: 0.8497 - val_loss: 0.1809 - val_rmse: 0.4253 - val_mape: 43.6277 - val_smape: 0.5239\n",
      "Epoch 17/20\n",
      "8/8 - 10s - loss: 0.3766 - rmse: 0.6136 - mape: 4992684.0000 - smape: 0.8577 - val_loss: 0.2015 - val_rmse: 0.4489 - val_mape: 46.0436 - val_smape: 0.5612\n",
      "Epoch 18/20\n",
      "8/8 - 10s - loss: 0.3595 - rmse: 0.5996 - mape: 4491665.0000 - smape: 0.8386 - val_loss: 0.1753 - val_rmse: 0.4187 - val_mape: 42.9096 - val_smape: 0.5134\n",
      "Epoch 19/20\n",
      "8/8 - 10s - loss: 0.3559 - rmse: 0.5966 - mape: 4663554.0000 - smape: 0.8333 - val_loss: 0.1788 - val_rmse: 0.4228 - val_mape: 43.3108 - val_smape: 0.5196\n",
      "Epoch 20/20\n",
      "8/8 - 10s - loss: 0.3618 - rmse: 0.6015 - mape: 4701389.5000 - smape: 0.8303 - val_loss: 0.1988 - val_rmse: 0.4459 - val_mape: 45.6662 - val_smape: 0.5559\n",
      "Test MAPE: 638.297\n",
      "Test sMAPE: 149.386\n",
      "Test RMSE: 128.858\n",
      "{'mape': 638.2968425750732, 'smape': 149.38639322916666, 'rmse': 128.85818}\n",
      "Epoch 1/20\n",
      "8/8 - 17s - loss: 2.2403 - rmse: 1.4968 - mape: 95701.2891 - smape: 1.1023 - val_loss: 0.3760 - val_rmse: 0.6132 - val_mape: 63.0667 - val_smape: 0.4611\n",
      "Epoch 2/20\n",
      "8/8 - 10s - loss: 1.2575 - rmse: 1.1214 - mape: 13158456.0000 - smape: 1.5574 - val_loss: 0.4708 - val_rmse: 0.6861 - val_mape: 70.4913 - val_smape: 1.0086\n",
      "Epoch 3/20\n",
      "8/8 - 10s - loss: 0.3045 - rmse: 0.5518 - mape: 2493251.5000 - smape: 0.8207 - val_loss: 0.0737 - val_rmse: 0.2715 - val_mape: 27.8051 - val_smape: 0.3049\n",
      "Epoch 4/20\n",
      "8/8 - 10s - loss: 0.4603 - rmse: 0.6785 - mape: 5758001.0000 - smape: 0.8289 - val_loss: 0.1461 - val_rmse: 0.3822 - val_mape: 39.2090 - val_smape: 0.4586\n",
      "Epoch 5/20\n",
      "8/8 - 10s - loss: 0.4358 - rmse: 0.6602 - mape: 4959474.0000 - smape: 0.9481 - val_loss: 0.2368 - val_rmse: 0.4866 - val_mape: 49.9668 - val_smape: 0.6235\n",
      "Epoch 6/20\n",
      "8/8 - 10s - loss: 0.3797 - rmse: 0.6162 - mape: 4019852.7500 - smape: 0.8993 - val_loss: 0.1770 - val_rmse: 0.4208 - val_mape: 43.1814 - val_smape: 0.5170\n",
      "Epoch 7/20\n",
      "8/8 - 10s - loss: 0.3847 - rmse: 0.6202 - mape: 4425232.0000 - smape: 0.8533 - val_loss: 0.1555 - val_rmse: 0.3943 - val_mape: 40.4554 - val_smape: 0.4766\n",
      "Epoch 8/20\n",
      "8/8 - 10s - loss: 0.4088 - rmse: 0.6394 - mape: 4353683.5000 - smape: 0.8793 - val_loss: 0.1890 - val_rmse: 0.4347 - val_mape: 44.6174 - val_smape: 0.5388\n",
      "Epoch 9/20\n",
      "8/8 - 10s - loss: 0.3885 - rmse: 0.6233 - mape: 4304984.0000 - smape: 0.8851 - val_loss: 0.1883 - val_rmse: 0.4339 - val_mape: 44.5344 - val_smape: 0.5375\n",
      "Epoch 10/20\n",
      "8/8 - 10s - loss: 0.3899 - rmse: 0.6244 - mape: 4634430.0000 - smape: 0.8774 - val_loss: 0.1805 - val_rmse: 0.4249 - val_mape: 43.6091 - val_smape: 0.5234\n",
      "Epoch 11/20\n",
      "8/8 - 10s - loss: 0.3938 - rmse: 0.6275 - mape: 4152420.5000 - smape: 0.8797 - val_loss: 0.1805 - val_rmse: 0.4248 - val_mape: 43.6026 - val_smape: 0.5233\n",
      "Epoch 12/20\n",
      "8/8 - 10s - loss: 0.3849 - rmse: 0.6204 - mape: 4485946.0000 - smape: 0.8711 - val_loss: 0.1846 - val_rmse: 0.4296 - val_mape: 44.1005 - val_smape: 0.5309\n",
      "Epoch 13/20\n",
      "8/8 - 10s - loss: 0.3911 - rmse: 0.6254 - mape: 4515654.0000 - smape: 0.8774 - val_loss: 0.1871 - val_rmse: 0.4326 - val_mape: 44.4006 - val_smape: 0.5355\n",
      "Epoch 14/20\n",
      "8/8 - 10s - loss: 0.3811 - rmse: 0.6173 - mape: 4343287.0000 - smape: 0.8682 - val_loss: 0.1823 - val_rmse: 0.4269 - val_mape: 43.8208 - val_smape: 0.5267\n",
      "Epoch 15/20\n",
      "8/8 - 10s - loss: 0.3866 - rmse: 0.6218 - mape: 4497547.5000 - smape: 0.8721 - val_loss: 0.1900 - val_rmse: 0.4359 - val_mape: 44.7429 - val_smape: 0.5407\n",
      "Epoch 16/20\n",
      "8/8 - 10s - loss: 0.3877 - rmse: 0.6227 - mape: 4510555.0000 - smape: 0.8752 - val_loss: 0.1917 - val_rmse: 0.4378 - val_mape: 44.9405 - val_smape: 0.5438\n",
      "Epoch 17/20\n",
      "8/8 - 10s - loss: 0.3855 - rmse: 0.6209 - mape: 4275075.0000 - smape: 0.8677 - val_loss: 0.1892 - val_rmse: 0.4349 - val_mape: 44.6368 - val_smape: 0.5391\n",
      "Epoch 18/20\n",
      "8/8 - 10s - loss: 0.3853 - rmse: 0.6207 - mape: 4711756.5000 - smape: 0.8632 - val_loss: 0.1917 - val_rmse: 0.4378 - val_mape: 44.9326 - val_smape: 0.5437\n",
      "Epoch 19/20\n",
      "8/8 - 10s - loss: 0.3818 - rmse: 0.6179 - mape: 4825531.5000 - smape: 0.8645 - val_loss: 0.1968 - val_rmse: 0.4437 - val_mape: 45.5335 - val_smape: 0.5530\n",
      "Epoch 20/20\n",
      "8/8 - 10s - loss: 0.3758 - rmse: 0.6130 - mape: 4054041.0000 - smape: 0.8623 - val_loss: 0.1936 - val_rmse: 0.4400 - val_mape: 45.1501 - val_smape: 0.5471\n",
      "Test MAPE: 630.870\n",
      "Test sMAPE: 149.073\n",
      "Test RMSE: 127.154\n",
      "{'mape': 630.8699131011963, 'smape': 149.0730997721354, 'rmse': 127.153534}\n",
      "Epoch 1/20\n",
      "8/8 - 17s - loss: 1.6445 - rmse: 1.2824 - mape: 257564.0469 - smape: 1.2356 - val_loss: 0.0914 - val_rmse: 0.3022 - val_mape: 31.0186 - val_smape: 0.2567\n",
      "Epoch 2/20\n",
      "8/8 - 10s - loss: 0.9665 - rmse: 0.9831 - mape: 10572054.0000 - smape: 1.2446 - val_loss: 0.3840 - val_rmse: 0.6197 - val_mape: 63.6604 - val_smape: 0.8685\n",
      "Epoch 3/20\n",
      "8/8 - 10s - loss: 0.3753 - rmse: 0.6126 - mape: 2807928.2500 - smape: 0.9632 - val_loss: 0.1189 - val_rmse: 0.3448 - val_mape: 35.3469 - val_smape: 0.4043\n",
      "Epoch 4/20\n",
      "8/8 - 10s - loss: 0.4206 - rmse: 0.6486 - mape: 5358747.0000 - smape: 0.8115 - val_loss: 0.1170 - val_rmse: 0.3420 - val_mape: 35.0642 - val_smape: 0.4004\n",
      "Epoch 5/20\n",
      "8/8 - 10s - loss: 0.4338 - rmse: 0.6587 - mape: 5006222.5000 - smape: 0.8996 - val_loss: 0.2149 - val_rmse: 0.4636 - val_mape: 47.5860 - val_smape: 0.5851\n",
      "Epoch 6/20\n",
      "8/8 - 10s - loss: 0.3883 - rmse: 0.6231 - mape: 3984472.2500 - smape: 0.9098 - val_loss: 0.1994 - val_rmse: 0.4465 - val_mape: 45.8294 - val_smape: 0.5575\n",
      "Epoch 7/20\n",
      "8/8 - 10s - loss: 0.3737 - rmse: 0.6113 - mape: 4413130.5000 - smape: 0.8582 - val_loss: 0.1629 - val_rmse: 0.4036 - val_mape: 41.4123 - val_smape: 0.4907\n",
      "Epoch 8/20\n",
      "8/8 - 10s - loss: 0.3938 - rmse: 0.6275 - mape: 4602392.0000 - smape: 0.8608 - val_loss: 0.1780 - val_rmse: 0.4219 - val_mape: 43.2931 - val_smape: 0.5187\n",
      "Epoch 9/20\n",
      "8/8 - 10s - loss: 0.3904 - rmse: 0.6248 - mape: 4649739.5000 - smape: 0.8782 - val_loss: 0.1931 - val_rmse: 0.4394 - val_mape: 45.0987 - val_smape: 0.5463\n",
      "Epoch 10/20\n",
      "8/8 - 10s - loss: 0.3783 - rmse: 0.6150 - mape: 4167777.7500 - smape: 0.8691 - val_loss: 0.1795 - val_rmse: 0.4236 - val_mape: 43.4675 - val_smape: 0.5214\n",
      "Epoch 11/20\n",
      "8/8 - 10s - loss: 0.3820 - rmse: 0.6181 - mape: 4714199.0000 - smape: 0.8528 - val_loss: 0.1749 - val_rmse: 0.4182 - val_mape: 42.9063 - val_smape: 0.5130\n",
      "Epoch 12/20\n",
      "8/8 - 10s - loss: 0.3850 - rmse: 0.6205 - mape: 4608756.5000 - smape: 0.8590 - val_loss: 0.1837 - val_rmse: 0.4286 - val_mape: 43.9693 - val_smape: 0.5291\n",
      "Epoch 13/20\n",
      "8/8 - 10s - loss: 0.3783 - rmse: 0.6150 - mape: 4315494.5000 - smape: 0.8540 - val_loss: 0.1838 - val_rmse: 0.4287 - val_mape: 43.9758 - val_smape: 0.5293\n",
      "Epoch 14/20\n",
      "8/8 - 10s - loss: 0.3763 - rmse: 0.6135 - mape: 4742131.0000 - smape: 0.8485 - val_loss: 0.1826 - val_rmse: 0.4273 - val_mape: 43.8286 - val_smape: 0.5271\n",
      "Epoch 15/20\n",
      "8/8 - 10s - loss: 0.3710 - rmse: 0.6091 - mape: 4603605.5000 - smape: 0.8463 - val_loss: 0.1849 - val_rmse: 0.4300 - val_mape: 44.0775 - val_smape: 0.5310\n",
      "Epoch 16/20\n",
      "8/8 - 10s - loss: 0.3616 - rmse: 0.6013 - mape: 4210595.0000 - smape: 0.8328 - val_loss: 0.1732 - val_rmse: 0.4161 - val_mape: 42.6313 - val_smape: 0.5093\n",
      "Epoch 17/20\n",
      "8/8 - 10s - loss: 0.3645 - rmse: 0.6038 - mape: 4556137.0000 - smape: 0.8237 - val_loss: 0.1897 - val_rmse: 0.4355 - val_mape: 44.6000 - val_smape: 0.5394\n",
      "Epoch 18/20\n",
      "8/8 - 10s - loss: 0.3416 - rmse: 0.5845 - mape: 4407295.0000 - smape: 0.8038 - val_loss: 0.1745 - val_rmse: 0.4178 - val_mape: 42.7283 - val_smape: 0.5113\n",
      "Epoch 19/20\n",
      "8/8 - 10s - loss: 0.3523 - rmse: 0.5935 - mape: 4355976.5000 - smape: 0.8070 - val_loss: 0.1988 - val_rmse: 0.4459 - val_mape: 45.5614 - val_smape: 0.5552\n",
      "Epoch 20/20\n",
      "8/8 - 10s - loss: 0.3364 - rmse: 0.5800 - mape: 4548174.5000 - smape: 0.7988 - val_loss: 0.1843 - val_rmse: 0.4292 - val_mape: 43.7668 - val_smape: 0.5282\n",
      "Test MAPE: 613.123\n",
      "Test sMAPE: 147.500\n",
      "Test RMSE: 124.051\n",
      "{'mape': 613.1228923797607, 'smape': 147.50043741861978, 'rmse': 124.05146}\n",
      "Epoch 1/20\n",
      "8/8 - 17s - loss: 1.8511 - rmse: 1.3605 - mape: 2539836.2500 - smape: 1.2111 - val_loss: 0.1999 - val_rmse: 0.4471 - val_mape: 45.9744 - val_smape: 0.3585\n",
      "Epoch 2/20\n",
      "8/8 - 10s - loss: 1.0383 - rmse: 1.0190 - mape: 10533963.0000 - smape: 1.3084 - val_loss: 0.3838 - val_rmse: 0.6195 - val_mape: 63.6418 - val_smape: 0.8681\n",
      "Epoch 3/20\n",
      "8/8 - 10s - loss: 0.3389 - rmse: 0.5822 - mape: 2467752.5000 - smape: 0.8609 - val_loss: 0.1206 - val_rmse: 0.3473 - val_mape: 35.6017 - val_smape: 0.4079\n",
      "Epoch 4/20\n",
      "8/8 - 10s - loss: 0.4285 - rmse: 0.6546 - mape: 4640734.5000 - smape: 0.8372 - val_loss: 0.1633 - val_rmse: 0.4041 - val_mape: 41.4696 - val_smape: 0.4915\n",
      "Epoch 5/20\n",
      "8/8 - 10s - loss: 0.4223 - rmse: 0.6498 - mape: 4993855.0000 - smape: 0.9206 - val_loss: 0.2302 - val_rmse: 0.4798 - val_mape: 49.2661 - val_smape: 0.6121\n",
      "Epoch 6/20\n",
      "8/8 - 10s - loss: 0.3897 - rmse: 0.6242 - mape: 4231893.5000 - smape: 0.9117 - val_loss: 0.1631 - val_rmse: 0.4038 - val_mape: 41.4379 - val_smape: 0.4910\n",
      "Epoch 7/20\n",
      "8/8 - 10s - loss: 0.4043 - rmse: 0.6359 - mape: 5184526.0000 - smape: 0.8425 - val_loss: 0.1639 - val_rmse: 0.4049 - val_mape: 41.5506 - val_smape: 0.4927\n",
      "Epoch 8/20\n",
      "8/8 - 10s - loss: 0.4089 - rmse: 0.6394 - mape: 4944277.0000 - smape: 0.8827 - val_loss: 0.2258 - val_rmse: 0.4752 - val_mape: 48.7867 - val_smape: 0.6043\n",
      "Epoch 9/20\n",
      "8/8 - 10s - loss: 0.3873 - rmse: 0.6223 - mape: 4671526.0000 - smape: 0.8855 - val_loss: 0.2342 - val_rmse: 0.4839 - val_mape: 49.6772 - val_smape: 0.6188\n",
      "Epoch 10/20\n",
      "8/8 - 10s - loss: 0.3905 - rmse: 0.6249 - mape: 4675779.5000 - smape: 0.8705 - val_loss: 0.2289 - val_rmse: 0.4784 - val_mape: 49.1130 - val_smape: 0.6096\n",
      "Epoch 11/20\n",
      "8/8 - 10s - loss: 0.3861 - rmse: 0.6214 - mape: 4573089.0000 - smape: 0.8766 - val_loss: 0.2155 - val_rmse: 0.4643 - val_mape: 47.6631 - val_smape: 0.5863\n",
      "Epoch 12/20\n",
      "8/8 - 10s - loss: 0.3946 - rmse: 0.6282 - mape: 4845344.5000 - smape: 0.8897 - val_loss: 0.1913 - val_rmse: 0.4374 - val_mape: 44.8979 - val_smape: 0.5431\n",
      "Epoch 13/20\n",
      "8/8 - 10s - loss: 0.3691 - rmse: 0.6075 - mape: 4337897.5000 - smape: 0.8432 - val_loss: 0.1794 - val_rmse: 0.4236 - val_mape: 43.4772 - val_smape: 0.5214\n",
      "Epoch 14/20\n",
      "8/8 - 10s - loss: 0.3895 - rmse: 0.6241 - mape: 4986567.5000 - smape: 0.8365 - val_loss: 0.2358 - val_rmse: 0.4856 - val_mape: 49.8364 - val_smape: 0.6215\n",
      "Epoch 15/20\n",
      "8/8 - 10s - loss: 0.3834 - rmse: 0.6192 - mape: 5016852.5000 - smape: 0.8804 - val_loss: 0.2795 - val_rmse: 0.5287 - val_mape: 54.2818 - val_smape: 0.6962\n",
      "Epoch 16/20\n",
      "8/8 - 10s - loss: 0.3617 - rmse: 0.6014 - mape: 4776059.5000 - smape: 0.8566 - val_loss: 0.2643 - val_rmse: 0.5141 - val_mape: 52.7805 - val_smape: 0.6704\n",
      "Epoch 17/20\n",
      "8/8 - 10s - loss: 0.3572 - rmse: 0.5976 - mape: 4667747.0000 - smape: 0.8354 - val_loss: 0.2580 - val_rmse: 0.5079 - val_mape: 52.1496 - val_smape: 0.6597\n",
      "Epoch 18/20\n",
      "8/8 - 11s - loss: 0.3631 - rmse: 0.6026 - mape: 4928274.0000 - smape: 0.8367 - val_loss: 0.2718 - val_rmse: 0.5214 - val_mape: 53.5430 - val_smape: 0.6833\n",
      "Epoch 19/20\n",
      "8/8 - 12s - loss: 0.3506 - rmse: 0.5921 - mape: 4487282.5000 - smape: 0.8192 - val_loss: 0.2709 - val_rmse: 0.5205 - val_mape: 53.4391 - val_smape: 0.6817\n",
      "Epoch 20/20\n",
      "8/8 - 13s - loss: 0.3464 - rmse: 0.5886 - mape: 4030151.7500 - smape: 0.8178 - val_loss: 0.2822 - val_rmse: 0.5313 - val_mape: 54.5613 - val_smape: 0.7009\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f9a3ee17c20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Test MAPE: 760.678\n",
      "Test sMAPE: 155.876\n",
      "Test RMSE: 153.531\n",
      "{'mape': 760.6775760650635, 'smape': 155.8758544921875, 'rmse': 153.5315}\n",
      "rmse : average=132.166, std=10.795\n",
      "mape : average=654.901, std=53.541\n",
      "smape : average=150.192, std=2.919\n"
     ]
    }
   ],
   "source": [
    "calculate_mean_std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc8dc88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:generalml_p37_gpu_v1]",
   "language": "python",
   "name": "conda-env-generalml_p37_gpu_v1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
