{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62759a86",
   "metadata": {},
   "source": [
    "# Setup enviorment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6a0efa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data reading in Dataframe format and data preprocessing\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Linear algebra operations\n",
    "import numpy as np\n",
    "\n",
    "# Image processing\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "\n",
    "# Machine learning models and preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import Sequential, layers, callbacks\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional, Flatten\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Epiweek\n",
    "from epiweeks import Week, Year\n",
    "\n",
    "# Date\n",
    "from datetime import date as convert_to_date\n",
    "\n",
    "# Os\n",
    "import os\n",
    "\n",
    "# Feature Extraction Model:\n",
    "from Variational_Autoencoder_Architecture import get_Variational_Autoencoder\n",
    "\n",
    "tf.compat.v1.experimental.output_all_intermediates(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "516ae6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c48b571f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = 'DATASET_5_best_cities/'\n",
    "labels = 'Tabular_data/Label_CSV_All_Municipality.csv'\n",
    "MUNICIPALITY = 'Ibagué'\n",
    "\n",
    "target_size = (224, 224, 12)\n",
    "backbone = 'Models/vae_224_1024.h5'\n",
    "\n",
    "cities =  {\n",
    "  \"76001\": \"Cali\",\n",
    "  \"05001\": \"Medellín\",\n",
    "  \"50001\": \"Villavicencio\",\n",
    "  \"54001\": \"Cúcuta\",\n",
    "  \"73001\": \"Ibagué\",\n",
    "  \"68001\": \"Bucaramanga\",\n",
    "  \"05360\": \"Itagüí\",\n",
    "  \"08001\": \"Barranquilla\",\n",
    "  \"41001\": \"Neiva\",\n",
    "  \"23001\": \"Montería\"\n",
    "}\n",
    "\n",
    "codes =  {\n",
    "  \"Cali\": \"76001\",\n",
    "  \"Medellín\": \"05001\",\n",
    "  \"Villavicencio\": \"50001\",\n",
    "  \"Cúcuta\": \"54001\",\n",
    "  \"Ibagué\": \"73001\",\n",
    "  \"Bucaramanga\": \"68001\",\n",
    "  \"Itagüí\": \"05360\",\n",
    "  \"Barranquilla\": \"08001\",\n",
    "  \"Neiva\": \"41001\",\n",
    "  \"Montería\": \"23001\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c8adba",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f6474a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epiweek_from_date(image_date):\n",
    "    date = image_date.split('-')\n",
    "    \n",
    "    # Get year as int\n",
    "    year = ''.join(filter(str.isdigit, date[0]))\n",
    "    year = int(year)\n",
    "    \n",
    "    # Get month as int\n",
    "    month = ''.join(filter(str.isdigit, date[1]))\n",
    "    month = int(month)\n",
    "    \n",
    "    # Get day as int\n",
    "    day = ''.join(filter(str.isdigit, date[2]))\n",
    "    day = int(day)\n",
    "    \n",
    "    # Get epiweek:\n",
    "    date = convert_to_date(year, month, day)\n",
    "    epiweek = str(Week.fromdate(date))\n",
    "    epiweek = int(epiweek)\n",
    "    \n",
    "    return epiweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fca65f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epiweek(name):\n",
    "    \n",
    "    # Get week\n",
    "    week = name.split('/')[1]\n",
    "    week = week.replace('w','')\n",
    "    week = int(week)\n",
    "    \n",
    "    # Year\n",
    "    year = name.split('/')[0]\n",
    "    year = int(year)\n",
    "    \n",
    "    epiweek = Week(year, week)\n",
    "    \n",
    "    epiweek = str(epiweek)\n",
    "    epiweek = int(epiweek)\n",
    "\n",
    "    return epiweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "657f3d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_labels(path, Municipality = None):\n",
    "    df = pd.read_csv(path)\n",
    "    if df.shape[1] > 678:\n",
    "        df = pd.concat([df[['Municipality code', 'Municipality']], df.iloc[:,-676:]], axis=1)\n",
    "        cols = df.iloc[:, 2:].columns\n",
    "        new_cols = df.iloc[:, 2:].columns.to_series().apply(get_epiweek)\n",
    "        df = df.rename(columns=dict(zip(cols, new_cols))) \n",
    "        \n",
    "    if 'Label_CSV_All_Municipality' in path:\n",
    "        # Get Columns\n",
    "        df = df[['epiweek', 'Municipality code', 'Municipality', 'final_cases_label']]\n",
    "        \n",
    "        # change epiweek format\n",
    "        df.epiweek = df.epiweek.apply(get_epiweek)\n",
    "        \n",
    "        # Remove duplicates\n",
    "        df = df[df.duplicated(['epiweek','Municipality code','Municipality']) == False]\n",
    "        \n",
    "        # Replace Increase, decrease, stable to numerical:\n",
    "        \"\"\"\n",
    "        - Stable = 0\n",
    "        - Increased = 1 \n",
    "        - Decreased = 2\n",
    "        \"\"\"\n",
    "        df.final_cases_label = df.final_cases_label.replace({'Stable': 0, 'Increased': 1, 'Decreased': 2})\n",
    "        \n",
    "        # Create table\n",
    "        df = df.pivot(index=['Municipality code', 'Municipality'], columns='epiweek', values='final_cases_label')\n",
    "\n",
    "        # Reset Index:\n",
    "        df = df.reset_index()\n",
    "    \n",
    "    if Municipality:\n",
    "        df = df[df['Municipality'] == Municipality]\n",
    "        df.drop(columns=['Municipality code'], inplace=True)\n",
    "        df.rename(columns={'Municipality': 'Municipality Code'}, inplace=True)\n",
    "    \n",
    "        df = df.set_index('Municipality Code')\n",
    "        df = df.T\n",
    "\n",
    "        df.columns.name = None\n",
    "        df.index.name = None\n",
    "        \n",
    "        df.columns = ['Cases']\n",
    "    \n",
    "    #df = df.reset_index()\n",
    "    #df.rename(columns={'index': 'epiweek'}, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d0d77e",
   "metadata": {},
   "source": [
    "### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d806621c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>201601</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201602</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201603</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201604</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201605</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201848</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201849</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201850</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201851</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201852</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>156 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0  1  2\n",
       "201601  1  0  0\n",
       "201602  1  0  0\n",
       "201603  0  1  0\n",
       "201604  0  0  1\n",
       "201605  0  1  0\n",
       "...    .. .. ..\n",
       "201848  0  1  0\n",
       "201849  0  0  1\n",
       "201850  1  0  0\n",
       "201851  1  0  0\n",
       "201852  1  0  0\n",
       "\n",
       "[156 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df = read_labels(path=labels, Municipality=MUNICIPALITY)\n",
    "labels_df_orig = labels_df\n",
    "labels_df = pd.get_dummies(labels_df['Cases'])\n",
    "labels_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e144f592",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1682b46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(images_dir):\n",
    "    \n",
    "    out_df = {\n",
    "        'epiweek':[],\n",
    "        'image':[]\n",
    "    }\n",
    "    \n",
    "    for image_path in os.listdir(images_dir):\n",
    "        if image_path.endswith('.tiff'):\n",
    "            epiweek = epiweek_from_date(image_path)\n",
    "            full_path = os.path.join(images_dir, image_path)\n",
    "            \n",
    "            out_df['epiweek'].append(epiweek)\n",
    "            out_df['image'].append(full_path)\n",
    "\n",
    "    df = pd.DataFrame(out_df)\n",
    "    \n",
    "    df = df.set_index('epiweek')\n",
    "    df.index.name = None\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d410676f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>201731</th>\n",
       "      <td>DATASET_5_best_cities/Ibagué/image_2017-07-30....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201551</th>\n",
       "      <td>DATASET_5_best_cities/Ibagué/image_2015-12-20....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201747</th>\n",
       "      <td>DATASET_5_best_cities/Ibagué/image_2017-11-19....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201647</th>\n",
       "      <td>DATASET_5_best_cities/Ibagué/image_2016-11-20....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201729</th>\n",
       "      <td>DATASET_5_best_cities/Ibagué/image_2017-07-16....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    image\n",
       "201731  DATASET_5_best_cities/Ibagué/image_2017-07-30....\n",
       "201551  DATASET_5_best_cities/Ibagué/image_2015-12-20....\n",
       "201747  DATASET_5_best_cities/Ibagué/image_2017-11-19....\n",
       "201647  DATASET_5_best_cities/Ibagué/image_2016-11-20....\n",
       "201729  DATASET_5_best_cities/Ibagué/image_2017-07-16...."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if 'DATASET_5_best_cities' in features:\n",
    "    MUNICIPALITY = MUNICIPALITY\n",
    "else:\n",
    "    MUNICIPALITY = codes[MUNICIPALITY]\n",
    "    \n",
    "images_dir = os.path.join(features, MUNICIPALITY)\n",
    "\n",
    "features_df = create_df(images_dir)\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1a05b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(path, target_size):\n",
    "    # Read the image and convert to numpy array\n",
    "    image = io.imread(path)\n",
    "    # Resize the image and normalize values\n",
    "    image_arr = resize(image,(target_size[0], target_size[1]))\n",
    "    #print(f'The shape of the image before reshape: {image_arr.shape}, of type{type(image_arr)}')\n",
    "\n",
    "    # Select RGB bands\n",
    "    if target_size[2] == 3:\n",
    "        image_arr = image_arr[:,:, [1,2,3]]\n",
    "    return image_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c2b6409",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.image = features_df.image.apply(read_image, args=[target_size])\n",
    "#features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdda122",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27165953",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = labels_df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd1809b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two dataframes based on the date values\n",
    "dengue_df = features_df.merge(labels_df, how='inner', left_index=True, right_index=True)\n",
    "dengue_df = dengue_df.sort_index()\n",
    "#dengue_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01d45ea",
   "metadata": {},
   "source": [
    "### Train Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f962f912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, train_percentage = 80):\n",
    "    # We need a sequence so we can't split randomly\n",
    "    # To divide into Train and test we have to calculate the train percentage of the dataset:\n",
    "    size = df.shape[0]\n",
    "    split = int(size*(train_percentage/100))\n",
    "    \n",
    "    \"\"\" Train \"\"\"\n",
    "    # We will train with 1st percentage % of data and test with the rest\n",
    "    train_df = df.iloc[:split,:] ## percentage % train\n",
    "    \n",
    "    \"\"\" Test \"\"\"\n",
    "    test_df = df.iloc[split:,:] # 100 - percentage % test\n",
    "    \n",
    "    print(f'The train shape is: {train_df.shape}')\n",
    "    print(f'The test shape is: {test_df.shape}')\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "669255a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train shape is: (124, 4)\n",
      "The test shape is: (32, 4)\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(dengue_df, train_percentage = 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b2a450",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3057a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_train_labels(df, column, feature_range=(-1, 1)):\n",
    "    # Get values of the column\n",
    "    values = df[column].values.reshape(-1,1)\n",
    "    # Generate a new scaler\n",
    "    scaler = MinMaxScaler(feature_range=feature_range)\n",
    "    # Fit the scaler just for that column\n",
    "    scaled_column = scaler.fit_transform(values)\n",
    "    # Add the scaled column to the dataframe\n",
    "    scaled_column = np.reshape(scaled_column, len(scaled_column))\n",
    "    df[column] = scaled_column\n",
    "    return df, scaler\n",
    "    \n",
    "def normalize_test_labels(df, column, scaler):\n",
    "    # Get values of the column\n",
    "    values = df[column].values.reshape(-1,1)\n",
    "    # Scale values\n",
    "    scaled_column = scaler.transform(values)\n",
    "    scaled_column = np.reshape(scaled_column,len(scaled_column))\n",
    "    # Add the scaled values to the df\n",
    "    df[column] = scaled_column\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd0edbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_range = (-1, 1)\n",
    "\n",
    "# Scale train:\n",
    "#train_df, scaler = normalize_train_labels(train_df, 'Cases', feature_range=feature_range)\n",
    "\n",
    "#train_df['Cases'].head()\n",
    "\n",
    "# Scale test:\n",
    "#test_df = normalize_test_labels(test_df, 'Cases', scaler=scaler)\n",
    "#test_df['Cases'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b76bb58",
   "metadata": {},
   "source": [
    "### Prepare data for time series supervised learning (function to create sliding window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "758632a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for time series\n",
    "\n",
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True, no_autoregressive=None):\n",
    "    if no_autoregressive:\n",
    "        n_in = n_in - 1\n",
    "        \n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        if no_autoregressive:\n",
    "            cols.append(df.shift(i).iloc[:,:-n_labels])\n",
    "            names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars-n_labels)]\n",
    "        else:\n",
    "            cols.append(df.shift(i))\n",
    "            names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "462c74b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of window\n",
    "days = 10\n",
    "no_autoregressive = True\n",
    "\n",
    "# frame as supervised learning\n",
    "train = series_to_supervised(train_df, n_in=days, no_autoregressive=no_autoregressive)\n",
    "test = series_to_supervised(test_df, n_in=days, no_autoregressive=no_autoregressive)\n",
    "\n",
    "#DataFrame(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e97c44af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_df_to_np(train):\n",
    "    for i, column in enumerate(train.columns):\n",
    "        if i == 0:\n",
    "            train_arr = np.array(train[column].to_list())\n",
    "            train_arr = np.expand_dims(train_arr, axis=1)\n",
    "\n",
    "        else:\n",
    "            #print(f'original: {train_arr.shape}')\n",
    "\n",
    "            train_arr_aux = np.array(train[column].to_list())\n",
    "            train_arr_aux = np.expand_dims(train_arr_aux, axis=1)\n",
    "\n",
    "            #print(f'aux: {train_arr_aux.shape}')\n",
    "\n",
    "            train_arr = np.concatenate((train_arr, train_arr_aux), axis=1)\n",
    "\n",
    "    return train_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cb211e",
   "metadata": {},
   "source": [
    "### Features and Labels Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ad06ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_labels_set(timeseries_data, original_df):\n",
    "    \n",
    "    \"\"\" Features \"\"\"\n",
    "    # We define the number of features as (Cases and media cloud)\n",
    "    n_features = original_df.shape[1]\n",
    "\n",
    "    # The features to train the model will be all except the values of the actual week \n",
    "    # We can't use other variables in week t because whe need to resample a a 3D Array\n",
    "    features_set = DataFrame(timeseries_data.values[:,:-n_labels])\n",
    "    # Convert pandas data frame to np.array to reshape as 3D Array\n",
    "    features_set = convert_df_to_np(features_set)\n",
    "    print(f'The shape of the features is {features_set.shape}')\n",
    "    \n",
    "    \"\"\" Labels \"\"\"\n",
    "    # We will use Covid cases in last week \n",
    "    labels_set = DataFrame(timeseries_data.values[:,-n_labels:])\n",
    "    # Convert pandas data frame to np.array\n",
    "    labels_set = labels_set.to_numpy()\n",
    "    print(f'The shape of the labels is {labels_set.shape}')\n",
    "    \n",
    "    return features_set, labels_set, n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19c12b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "The shape of the features is (115, 10, 224, 224, 12)\n",
      "The shape of the labels is (115, 3)\n",
      "Test:\n",
      "The shape of the features is (23, 10, 224, 224, 12)\n",
      "The shape of the labels is (23, 3)\n"
     ]
    }
   ],
   "source": [
    "# Train features and labels set\n",
    "print('Train:')\n",
    "train_X, train_y, n_features = features_labels_set(timeseries_data=train, original_df=dengue_df)\n",
    "\n",
    "# Test features and labels set\n",
    "print('Test:')\n",
    "test_X, test_y, n_features = features_labels_set(timeseries_data=test, original_df=dengue_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7c3300e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.asarray(train_X).astype(np.float32)\n",
    "train_y = np.asarray(train_y).astype(np.float32)\n",
    "\n",
    "test_X = np.asarray(test_X).astype(np.float32)\n",
    "test_y = np.asarray(test_y).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450453de",
   "metadata": {},
   "source": [
    "# Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e9f5187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seed\n",
    "#tf.random.set_seed(0)\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    epsilon = 0.1\n",
    "    summ = K.maximum(K.abs(y_true) + K.abs(y_pred) + epsilon, 0.5 + epsilon)\n",
    "    smape = K.abs(y_pred - y_true) / summ * 2.0\n",
    "    return smape\n",
    "\n",
    "def create_model(backbone=backbone):\n",
    "    lstm_week, input_shape = days, target_size\n",
    "    \n",
    "    # design network\n",
    "    model = Sequential()\n",
    "\n",
    "    # CNN\n",
    "    cnn = get_Variational_Autoencoder(model_path=backbone, backbone=True)\n",
    "\n",
    "    for idx, layer in enumerate(cnn.layers):\n",
    "        layer.trainable = False # idx > len(cnn.layers) - 2 \n",
    "    \n",
    "    # https://levelup.gitconnected.com/hands-on-practice-with-time-distributed-layers-using-tensorflow-c776a5d78e7e\n",
    "    model.add(keras.layers.TimeDistributed(cnn, input_shape = ((lstm_week,) + input_shape)))\n",
    "    model.add(keras.layers.TimeDistributed(Flatten()))\n",
    "    model.add(keras.layers.TimeDistributed(Dense(1024)))\n",
    "    model.add(LSTM(120, dropout=0.1, return_sequences=True))\n",
    "    model.add(LSTM(240, dropout=0.1, return_sequences = False))\n",
    "    model.add(Dense(60))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    #print(model.summary())\n",
    "    \n",
    "    # Compile the model:\n",
    "    opt = keras.optimizers.Adam()\n",
    "    metrics = [\n",
    "        tf.keras.metrics.AUC(name='auc', multi_label=True, num_labels=3),\n",
    "        tf.keras.metrics.CategoricalAccuracy(name='acc'),\n",
    "        tf.keras.metrics.CategoricalCrossentropy(name='entropy')\n",
    "    ]\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=metrics)\n",
    "    \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca586825",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d05bec14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# EarlyStopping:\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=20, \n",
    "        verbose=1, mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6112818e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zeros: 104, ones: 23, twos: 29, total: 156\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 1.5, 1: 6.782608695652174, 2: 5.379310344827586}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Imbalanced data\n",
    "n_zeros = (labels_df_orig.to_numpy() == 0).sum()\n",
    "n_ones = (labels_df_orig.to_numpy() == 1).sum()\n",
    "n_twos = (labels_df_orig.to_numpy() == 2).sum()\n",
    "n_total = n_zeros + n_ones + n_twos\n",
    "\n",
    "weights = {0: n_total/n_zeros, 1: n_total/n_ones, 2: n_total/n_twos}\n",
    "print(f'zeros: {n_zeros}, ones: {n_ones}, twos: {n_twos}, total: {n_total}')\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3697c8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit network\n",
    "def train_model(model, monitor, weights, plot=None, epochs=20):\n",
    "    if monitor and weights:\n",
    "        history = model.fit(train_X, train_y, epochs=epochs, batch_size=16, validation_data=(test_X, test_y), verbose=2, shuffle=False, callbacks=[monitor], class_weight=weights)\n",
    "    elif monitor:\n",
    "        history = model.fit(train_X, train_y, epochs=epochs, batch_size=16, validation_data=(test_X, test_y), verbose=2, shuffle=False, callbacks=[monitor])\n",
    "    elif weights:\n",
    "        history = model.fit(train_X, train_y, epochs=epochs, batch_size=16, validation_data=(test_X, test_y), verbose=2, shuffle=False, class_weight=weights)\n",
    "    else:\n",
    "        history = model.fit(train_X, train_y, epochs=epochs, batch_size=16, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "    \n",
    "    if plot:\n",
    "        # plot history\n",
    "        plt.plot(history.history['loss'], label='train')\n",
    "        plt.plot(history.history['val_loss'], label='validation')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ed47f9",
   "metadata": {},
   "source": [
    "# AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33076e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also evaluate or predict on a dataset.\n",
    "def evaluate(model, verbose = None):\n",
    "    if verbose:\n",
    "        print('Evaluate: ')\n",
    "    result = model.evaluate(test_X, test_y)\n",
    "    stored_results = {}\n",
    "    for i, metric in enumerate(model.metrics_names):\n",
    "        stored_results[metric] = result[i]\n",
    "        if verbose:\n",
    "            print(f'{metric}: {result[i]}')\n",
    "    return stored_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b443f42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(inv_y, inv_yhat, model_name = ''):\n",
    "    data_predict = inv_yhat  ## predicted target cases\n",
    "    dataY_plot = inv_y  ##  real test-target cases\n",
    "\n",
    "    data_predict = data_predict.reshape(len(data_predict), 1)\n",
    "    dataY_plot = dataY_plot.reshape(len(dataY_plot), 1)\n",
    "\n",
    "    plt.plot(dataY_plot, label = 'actual')\n",
    "    plt.plot(data_predict, label = 'predicted')\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.suptitle(f'Time-Series Prediction with {model_name}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a98146",
   "metadata": {},
   "source": [
    "# Calculate Mean and SD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f6a31c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_std(weights):\n",
    "    \n",
    "    metrics = {\n",
    "        \"auc\": [],\n",
    "        \"acc\": [],\n",
    "        \"entropy\": []\n",
    "    }\n",
    "    \n",
    "    for i in range(3):\n",
    "        model = create_model(backbone=backbone)\n",
    "        train_model(model=model, monitor=monitor, weights=weights)\n",
    "        stored_results = evaluate(model=model)\n",
    "        \n",
    "        for key in metrics.keys():\n",
    "            metrics[key].append(stored_results[key])\n",
    "            \n",
    "    for key in metrics.keys():\n",
    "        results = metrics[key]\n",
    "        print(key, f\": average={np.average(results):.3f}, std={np.std(results):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bec7808c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/datascience/conda/generalml_p37_gpu_v1/lib/python3.7/site-packages/keras/layers/normalization/batch_normalization.py:520: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed (TimeDistri (None, 10, 1024)          524391904 \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 10, 1024)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 10, 1024)          1049600   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 10, 120)           549600    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 240)               346560    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 60)                14460     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 183       \n",
      "=================================================================\n",
      "Total params: 526,352,307\n",
      "Trainable params: 1,960,403\n",
      "Non-trainable params: 524,391,904\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 115 samples, validate on 23 samples\n",
      "Epoch 1/20\n",
      "115/115 - 5s - loss: 0.9349 - auc: 0.6264 - acc: 0.5478 - entropy: 0.9349 - val_loss: 0.6241 - val_auc: 0.5268 - val_acc: 0.8261 - val_entropy: 0.6241\n",
      "Epoch 2/20\n",
      "115/115 - 3s - loss: 1.2045 - auc: 0.2894 - acc: 0.6609 - entropy: 1.2045 - val_loss: 0.7502 - val_auc: 0.5586 - val_acc: 0.7826 - val_entropy: 0.7502\n",
      "Epoch 3/20\n",
      "115/115 - 3s - loss: 0.9282 - auc: 0.4696 - acc: 0.6435 - entropy: 0.9282 - val_loss: 0.6556 - val_auc: 0.6846 - val_acc: 0.7826 - val_entropy: 0.6556\n",
      "Epoch 4/20\n",
      "115/115 - 4s - loss: 0.8672 - auc: 0.5753 - acc: 0.6609 - entropy: 0.8672 - val_loss: 0.5965 - val_auc: 0.5964 - val_acc: 0.8261 - val_entropy: 0.5965\n",
      "Epoch 5/20\n",
      "115/115 - 3s - loss: 0.9657 - auc: 0.3378 - acc: 0.6609 - entropy: 0.9657 - val_loss: 0.6975 - val_auc: 0.2601 - val_acc: 0.8261 - val_entropy: 0.6975\n",
      "Epoch 6/20\n",
      "115/115 - 3s - loss: 0.9146 - auc: 0.4495 - acc: 0.6609 - entropy: 0.9146 - val_loss: 0.6673 - val_auc: 0.5233 - val_acc: 0.8261 - val_entropy: 0.6673\n",
      "Epoch 7/20\n",
      "115/115 - 3s - loss: 0.8992 - auc: 0.4892 - acc: 0.6609 - entropy: 0.8992 - val_loss: 0.6080 - val_auc: 0.5977 - val_acc: 0.8261 - val_entropy: 0.6080\n",
      "Epoch 8/20\n",
      "115/115 - 3s - loss: 0.8899 - auc: 0.4979 - acc: 0.6609 - entropy: 0.8899 - val_loss: 0.6920 - val_auc: 0.2170 - val_acc: 0.8261 - val_entropy: 0.6920\n",
      "Epoch 9/20\n",
      "115/115 - 3s - loss: 0.8961 - auc: 0.5179 - acc: 0.6609 - entropy: 0.8961 - val_loss: 0.6436 - val_auc: 0.5315 - val_acc: 0.8261 - val_entropy: 0.6436\n",
      "Epoch 10/20\n",
      "115/115 - 4s - loss: 0.8977 - auc: 0.5275 - acc: 0.6609 - entropy: 0.8977 - val_loss: 0.5333 - val_auc: 0.8938 - val_acc: 0.8261 - val_entropy: 0.5333\n",
      "Epoch 11/20\n",
      "115/115 - 3s - loss: 0.9283 - auc: 0.4564 - acc: 0.6609 - entropy: 0.9283 - val_loss: 0.6625 - val_auc: 0.4353 - val_acc: 0.8261 - val_entropy: 0.6625\n",
      "Epoch 12/20\n",
      "115/115 - 3s - loss: 0.8660 - auc: 0.5487 - acc: 0.6522 - entropy: 0.8660 - val_loss: 0.6157 - val_auc: 0.5334 - val_acc: 0.8261 - val_entropy: 0.6157\n",
      "Epoch 13/20\n",
      "115/115 - 3s - loss: 0.9013 - auc: 0.4810 - acc: 0.6609 - entropy: 0.9013 - val_loss: 0.5941 - val_auc: 0.6798 - val_acc: 0.8261 - val_entropy: 0.5941\n",
      "Epoch 14/20\n",
      "115/115 - 3s - loss: 0.9066 - auc: 0.4641 - acc: 0.6609 - entropy: 0.9066 - val_loss: 0.5574 - val_auc: 0.7435 - val_acc: 0.8261 - val_entropy: 0.5574\n",
      "Epoch 15/20\n",
      "115/115 - 3s - loss: 0.8601 - auc: 0.5756 - acc: 0.6783 - entropy: 0.8601 - val_loss: 0.6232 - val_auc: 0.5278 - val_acc: 0.8261 - val_entropy: 0.6232\n",
      "Epoch 16/20\n",
      "115/115 - 3s - loss: 0.8494 - auc: 0.6319 - acc: 0.6609 - entropy: 0.8494 - val_loss: 0.6107 - val_auc: 0.5677 - val_acc: 0.8261 - val_entropy: 0.6107\n",
      "Epoch 17/20\n",
      "115/115 - 3s - loss: 0.9182 - auc: 0.4831 - acc: 0.6435 - entropy: 0.9182 - val_loss: 0.6302 - val_auc: 0.4607 - val_acc: 0.8261 - val_entropy: 0.6302\n",
      "Epoch 18/20\n",
      "115/115 - 3s - loss: 0.8782 - auc: 0.5429 - acc: 0.6609 - entropy: 0.8782 - val_loss: 0.6407 - val_auc: 0.4114 - val_acc: 0.8261 - val_entropy: 0.6407\n",
      "Epoch 19/20\n",
      "115/115 - 3s - loss: 0.8690 - auc: 0.5528 - acc: 0.6609 - entropy: 0.8690 - val_loss: 0.5971 - val_auc: 0.6496 - val_acc: 0.8261 - val_entropy: 0.5971\n",
      "Epoch 20/20\n",
      "115/115 - 3s - loss: 0.9119 - auc: 0.4792 - acc: 0.6522 - entropy: 0.9119 - val_loss: 0.5551 - val_auc: 0.8124 - val_acc: 0.8261 - val_entropy: 0.5551\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_3 (TimeDist (None, 10, 1024)          524391904 \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 10, 1024)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 10, 1024)          1049600   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 10, 120)           549600    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 240)               346560    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 60)                14460     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 3)                 183       \n",
      "=================================================================\n",
      "Total params: 526,352,307\n",
      "Trainable params: 1,960,403\n",
      "Non-trainable params: 524,391,904\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 115 samples, validate on 23 samples\n",
      "Epoch 1/20\n",
      "115/115 - 5s - loss: 1.0613 - auc: 0.5079 - acc: 0.4783 - entropy: 1.0613 - val_loss: 0.9431 - val_auc: 0.0924 - val_acc: 0.6957 - val_entropy: 0.9431\n",
      "Epoch 2/20\n",
      "115/115 - 4s - loss: 0.9319 - auc: 0.4923 - acc: 0.6435 - entropy: 0.9319 - val_loss: 0.6001 - val_auc: 0.5897 - val_acc: 0.8261 - val_entropy: 0.6001\n",
      "Epoch 3/20\n",
      "115/115 - 3s - loss: 0.9763 - auc: 0.4546 - acc: 0.6435 - entropy: 0.9763 - val_loss: 0.6055 - val_auc: 0.5949 - val_acc: 0.8261 - val_entropy: 0.6055\n",
      "Epoch 4/20\n",
      "115/115 - 3s - loss: 1.0083 - auc: 0.3672 - acc: 0.6261 - entropy: 1.0083 - val_loss: 0.6923 - val_auc: 0.6206 - val_acc: 0.8261 - val_entropy: 0.6923\n",
      "Epoch 5/20\n",
      "115/115 - 3s - loss: 0.8805 - auc: 0.5619 - acc: 0.6522 - entropy: 0.8805 - val_loss: 0.6136 - val_auc: 0.7073 - val_acc: 0.7826 - val_entropy: 0.6136\n",
      "Epoch 6/20\n",
      "115/115 - 3s - loss: 0.9825 - auc: 0.3617 - acc: 0.6522 - entropy: 0.9825 - val_loss: 0.6461 - val_auc: 0.4967 - val_acc: 0.8261 - val_entropy: 0.6460\n",
      "Epoch 7/20\n",
      "115/115 - 3s - loss: 0.8998 - auc: 0.4811 - acc: 0.6522 - entropy: 0.8998 - val_loss: 0.6126 - val_auc: 0.6229 - val_acc: 0.8261 - val_entropy: 0.6126\n",
      "Epoch 8/20\n",
      "115/115 - 3s - loss: 0.9232 - auc: 0.4354 - acc: 0.6522 - entropy: 0.9232 - val_loss: 0.7149 - val_auc: 0.2135 - val_acc: 0.8261 - val_entropy: 0.7149\n",
      "Epoch 9/20\n",
      "115/115 - 3s - loss: 0.8996 - auc: 0.4846 - acc: 0.6609 - entropy: 0.8996 - val_loss: 0.6483 - val_auc: 0.4621 - val_acc: 0.8261 - val_entropy: 0.6483\n",
      "Epoch 10/20\n",
      "115/115 - 3s - loss: 0.9029 - auc: 0.4774 - acc: 0.6609 - entropy: 0.9029 - val_loss: 0.6376 - val_auc: 0.4635 - val_acc: 0.8261 - val_entropy: 0.6376\n",
      "Epoch 11/20\n",
      "115/115 - 3s - loss: 0.9302 - auc: 0.3983 - acc: 0.6609 - entropy: 0.9302 - val_loss: 0.6810 - val_auc: 0.5863 - val_acc: 0.8261 - val_entropy: 0.6810\n",
      "Epoch 12/20\n",
      "115/115 - 3s - loss: 0.8827 - auc: 0.5272 - acc: 0.6609 - entropy: 0.8827 - val_loss: 0.6222 - val_auc: 0.6267 - val_acc: 0.8261 - val_entropy: 0.6222\n",
      "Epoch 13/20\n",
      "115/115 - 3s - loss: 0.9229 - auc: 0.4183 - acc: 0.6609 - entropy: 0.9229 - val_loss: 0.6253 - val_auc: 0.5577 - val_acc: 0.8261 - val_entropy: 0.6253\n",
      "Epoch 14/20\n",
      "115/115 - 3s - loss: 0.9116 - auc: 0.4567 - acc: 0.6522 - entropy: 0.9116 - val_loss: 0.6423 - val_auc: 0.4576 - val_acc: 0.8261 - val_entropy: 0.6423\n",
      "Epoch 15/20\n",
      "115/115 - 3s - loss: 0.9231 - auc: 0.4098 - acc: 0.6609 - entropy: 0.9231 - val_loss: 0.6782 - val_auc: 0.5613 - val_acc: 0.8261 - val_entropy: 0.6782\n",
      "Epoch 16/20\n",
      "115/115 - 3s - loss: 0.8608 - auc: 0.5697 - acc: 0.6609 - entropy: 0.8608 - val_loss: 0.6918 - val_auc: 0.2514 - val_acc: 0.8261 - val_entropy: 0.6918\n",
      "Epoch 17/20\n",
      "115/115 - 3s - loss: 0.9089 - auc: 0.4386 - acc: 0.6696 - entropy: 0.9089 - val_loss: 0.6557 - val_auc: 0.4613 - val_acc: 0.8261 - val_entropy: 0.6557\n",
      "Epoch 18/20\n",
      "115/115 - 3s - loss: 0.8887 - auc: 0.5088 - acc: 0.6609 - entropy: 0.8887 - val_loss: 0.6351 - val_auc: 0.5506 - val_acc: 0.8261 - val_entropy: 0.6351\n",
      "Epoch 19/20\n",
      "115/115 - 3s - loss: 0.9200 - auc: 0.4137 - acc: 0.6522 - entropy: 0.9200 - val_loss: 0.6962 - val_auc: 0.2971 - val_acc: 0.8261 - val_entropy: 0.6962\n",
      "Epoch 20/20\n",
      "115/115 - 3s - loss: 0.8897 - auc: 0.4892 - acc: 0.6609 - entropy: 0.8897 - val_loss: 0.6755 - val_auc: 0.2758 - val_acc: 0.8261 - val_entropy: 0.6755\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_6 (TimeDist (None, 10, 1024)          524391904 \n",
      "_________________________________________________________________\n",
      "time_distributed_7 (TimeDist (None, 10, 1024)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_8 (TimeDist (None, 10, 1024)          1049600   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 10, 120)           549600    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 240)               346560    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 60)                14460     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 3)                 183       \n",
      "=================================================================\n",
      "Total params: 526,352,307\n",
      "Trainable params: 1,960,403\n",
      "Non-trainable params: 524,391,904\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 115 samples, validate on 23 samples\n",
      "Epoch 1/20\n",
      "115/115 - 5s - loss: 1.0533 - auc: 0.5491 - acc: 0.4174 - entropy: 1.0533 - val_loss: 0.9016 - val_auc: 0.3155 - val_acc: 0.6957 - val_entropy: 0.9016\n",
      "Epoch 2/20\n",
      "115/115 - 4s - loss: 0.8881 - auc: 0.5952 - acc: 0.6609 - entropy: 0.8881 - val_loss: 0.7191 - val_auc: 0.4185 - val_acc: 0.8261 - val_entropy: 0.7191\n",
      "Epoch 3/20\n",
      "115/115 - 3s - loss: 0.9806 - auc: 0.4421 - acc: 0.6348 - entropy: 0.9806 - val_loss: 0.8818 - val_auc: 0.0787 - val_acc: 0.8261 - val_entropy: 0.8818\n",
      "Epoch 4/20\n",
      "115/115 - 3s - loss: 0.9170 - auc: 0.5183 - acc: 0.6261 - entropy: 0.9170 - val_loss: 0.7535 - val_auc: 0.5196 - val_acc: 0.7391 - val_entropy: 0.7535\n",
      "Epoch 5/20\n",
      "115/115 - 4s - loss: 0.9318 - auc: 0.5086 - acc: 0.6261 - entropy: 0.9318 - val_loss: 0.6006 - val_auc: 0.5563 - val_acc: 0.8696 - val_entropy: 0.6006\n",
      "Epoch 6/20\n",
      "115/115 - 3s - loss: 0.9340 - auc: 0.4425 - acc: 0.6609 - entropy: 0.9340 - val_loss: 0.7023 - val_auc: 0.3395 - val_acc: 0.8261 - val_entropy: 0.7023\n",
      "Epoch 7/20\n",
      "115/115 - 3s - loss: 0.8684 - auc: 0.5928 - acc: 0.6522 - entropy: 0.8684 - val_loss: 0.7191 - val_auc: 0.3331 - val_acc: 0.8261 - val_entropy: 0.7191\n",
      "Epoch 8/20\n",
      "115/115 - 3s - loss: 0.9355 - auc: 0.4465 - acc: 0.6609 - entropy: 0.9355 - val_loss: 0.6484 - val_auc: 0.5990 - val_acc: 0.8261 - val_entropy: 0.6484\n",
      "Epoch 9/20\n",
      "115/115 - 3s - loss: 0.8882 - auc: 0.5256 - acc: 0.6609 - entropy: 0.8882 - val_loss: 0.6836 - val_auc: 0.4770 - val_acc: 0.8261 - val_entropy: 0.6836\n",
      "Epoch 10/20\n",
      "115/115 - 3s - loss: 0.8779 - auc: 0.5600 - acc: 0.6609 - entropy: 0.8779 - val_loss: 0.6216 - val_auc: 0.4718 - val_acc: 0.8261 - val_entropy: 0.6216\n",
      "Epoch 11/20\n",
      "115/115 - 3s - loss: 0.9552 - auc: 0.3867 - acc: 0.6609 - entropy: 0.9552 - val_loss: 0.7147 - val_auc: 0.2883 - val_acc: 0.8261 - val_entropy: 0.7147\n",
      "Epoch 12/20\n",
      "115/115 - 3s - loss: 0.9159 - auc: 0.4441 - acc: 0.6609 - entropy: 0.9159 - val_loss: 0.6739 - val_auc: 0.4594 - val_acc: 0.8261 - val_entropy: 0.6739\n",
      "Epoch 13/20\n",
      "115/115 - 3s - loss: 0.8788 - auc: 0.5308 - acc: 0.6609 - entropy: 0.8788 - val_loss: 0.6812 - val_auc: 0.3160 - val_acc: 0.8261 - val_entropy: 0.6812\n",
      "Epoch 14/20\n",
      "115/115 - 3s - loss: 0.9116 - auc: 0.4499 - acc: 0.6609 - entropy: 0.9116 - val_loss: 0.6409 - val_auc: 0.4725 - val_acc: 0.8261 - val_entropy: 0.6409\n",
      "Epoch 15/20\n",
      "115/115 - 3s - loss: 0.9075 - auc: 0.4712 - acc: 0.6609 - entropy: 0.9075 - val_loss: 0.6674 - val_auc: 0.5284 - val_acc: 0.8261 - val_entropy: 0.6674\n",
      "Epoch 16/20\n",
      "115/115 - 3s - loss: 0.9002 - auc: 0.4580 - acc: 0.6609 - entropy: 0.9002 - val_loss: 0.6787 - val_auc: 0.3884 - val_acc: 0.8261 - val_entropy: 0.6787\n",
      "Epoch 17/20\n",
      "115/115 - 3s - loss: 0.9198 - auc: 0.3963 - acc: 0.6609 - entropy: 0.9198 - val_loss: 0.6584 - val_auc: 0.4340 - val_acc: 0.8261 - val_entropy: 0.6584\n",
      "Epoch 18/20\n",
      "115/115 - 3s - loss: 0.8790 - auc: 0.5187 - acc: 0.6609 - entropy: 0.8790 - val_loss: 0.6692 - val_auc: 0.3312 - val_acc: 0.8261 - val_entropy: 0.6692\n",
      "Epoch 19/20\n",
      "115/115 - 3s - loss: 0.8752 - auc: 0.5682 - acc: 0.6609 - entropy: 0.8752 - val_loss: 0.6184 - val_auc: 0.6636 - val_acc: 0.8261 - val_entropy: 0.6184\n",
      "Epoch 20/20\n",
      "115/115 - 3s - loss: 0.8916 - auc: 0.4833 - acc: 0.6609 - entropy: 0.8916 - val_loss: 0.6412 - val_auc: 0.4977 - val_acc: 0.8261 - val_entropy: 0.6412\n",
      "auc : average=0.403, std=0.150\n",
      "acc : average=0.826, std=0.000\n",
      "entropy : average=0.647, std=0.040\n"
     ]
    }
   ],
   "source": [
    "calculate_mean_std(weights=None)\n",
    "#calculate_mean_std(weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c45cb66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06041696",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b0375b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:generalml_p37_gpu_v1]",
   "language": "python",
   "name": "conda-env-generalml_p37_gpu_v1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
