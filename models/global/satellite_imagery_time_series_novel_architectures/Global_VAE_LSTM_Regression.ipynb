{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f42d7c5a",
   "metadata": {},
   "source": [
    "# Setup enviorment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29c047c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data reading in Dataframe format and data preprocessing\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Linear algebra operations\n",
    "import numpy as np\n",
    "\n",
    "# Image processing\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "\n",
    "# Machine learning models and preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import Sequential, layers, callbacks\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional, Flatten\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Epiweek\n",
    "from epiweeks import Week, Year\n",
    "\n",
    "# Date\n",
    "from datetime import date as convert_to_date\n",
    "\n",
    "# Os\n",
    "import os\n",
    "\n",
    "# Feature Extraction Model:\n",
    "from Variational_Autoencoder_Architecture import get_Variational_Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d3fb499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a01f5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = 'DATASET_5_best_cities/'\n",
    "labels = 'Tabular_data/dengue_tabular.csv'\n",
    "Municipalities = ['Medellín', 'Cali', 'Villavicencio', 'Cúcuta', 'Ibagué']\n",
    "\n",
    "target_size = (224, 224, 12)\n",
    "backbone = 'Models/vae_224_1024.h5'\n",
    "\n",
    "cities =  {\n",
    "  \"76001\": \"Cali\",\n",
    "  \"05001\": \"Medellín\",\n",
    "  \"50001\": \"Villavicencio\",\n",
    "  \"54001\": \"Cúcuta\",\n",
    "  \"73001\": \"Ibagué\",\n",
    "  \"68001\": \"Bucaramanga\",\n",
    "  \"05360\": \"Itagüí\",\n",
    "  \"08001\": \"Barranquilla\",\n",
    "  \"41001\": \"Neiva\",\n",
    "  \"23001\": \"Montería\"\n",
    "}\n",
    "\n",
    "codes =  {\n",
    "  \"Cali\": \"76001\",\n",
    "  \"Medellín\": \"05001\",\n",
    "  \"Villavicencio\": \"50001\",\n",
    "  \"Cúcuta\": \"54001\",\n",
    "  \"Ibagué\": \"73001\",\n",
    "  \"Bucaramanga\": \"68001\",\n",
    "  \"Itagüí\": \"05360\",\n",
    "  \"Barranquilla\": \"08001\",\n",
    "  \"Neiva\": \"41001\",\n",
    "  \"Montería\": \"23001\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8103aea",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20db3a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epiweek_from_date(image_date):\n",
    "    date = image_date.split('-')\n",
    "    \n",
    "    # Get year as int\n",
    "    year = ''.join(filter(str.isdigit, date[0]))\n",
    "    year = int(year)\n",
    "    \n",
    "    # Get month as int\n",
    "    month = ''.join(filter(str.isdigit, date[1]))\n",
    "    month = int(month)\n",
    "    \n",
    "    # Get day as int\n",
    "    day = ''.join(filter(str.isdigit, date[2]))\n",
    "    day = int(day)\n",
    "    \n",
    "    # Get epiweek:\n",
    "    date = convert_to_date(year, month, day)\n",
    "    epiweek = str(Week.fromdate(date))\n",
    "    epiweek = int(epiweek)\n",
    "    \n",
    "    return epiweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e96c0702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epiweek(name):\n",
    "    \n",
    "    # Get week\n",
    "    week = name.split('/')[1]\n",
    "    week = week.replace('w','')\n",
    "    week = int(week)\n",
    "    \n",
    "    # Year\n",
    "    year = name.split('/')[0]\n",
    "    year = int(year)\n",
    "    \n",
    "    epiweek = Week(year, week)\n",
    "    \n",
    "    epiweek = str(epiweek)\n",
    "    epiweek = int(epiweek)\n",
    "\n",
    "    return epiweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "298f1a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_labels(path, Municipality = None):\n",
    "    df = pd.read_csv(path)\n",
    "    if df.shape[1] > 678:\n",
    "        df = pd.concat([df[['Municipality code', 'Municipality']], df.iloc[:,-676:]], axis=1)\n",
    "        cols = df.iloc[:, 2:].columns\n",
    "        new_cols = df.iloc[:, 2:].columns.to_series().apply(get_epiweek)\n",
    "        df = df.rename(columns=dict(zip(cols, new_cols))) \n",
    "        \n",
    "    if 'Label_CSV_All_Municipality' in path:\n",
    "        # Get Columns\n",
    "        df = df[['epiweek', 'Municipality code', 'Municipality', 'final_cases_label']]\n",
    "        \n",
    "        # change epiweek format\n",
    "        df.epiweek = df.epiweek.apply(get_epiweek)\n",
    "        \n",
    "        # Remove duplicates\n",
    "        df = df[df.duplicated(['epiweek','Municipality code','Municipality']) == False]\n",
    "        \n",
    "        # Replace Increase, decrease, stable to numerical:\n",
    "        \"\"\"\n",
    "        - Stable = 0\n",
    "        - Increased = 1 \n",
    "        - Decreased = 2\n",
    "        \"\"\"\n",
    "        df.final_cases_label = df.final_cases_label.replace({'Stable': 0, 'Increased': 1, 'Decreased': 2})\n",
    "        \n",
    "        # Create table\n",
    "        df = df.pivot(index=['Municipality code', 'Municipality'], columns='epiweek', values='final_cases_label')\n",
    "\n",
    "        # Reset Index:\n",
    "        df = df.reset_index()\n",
    "    \n",
    "    if Municipality:\n",
    "        df = df[df['Municipality'] == Municipality]\n",
    "        df.drop(columns=['Municipality code'], inplace=True)\n",
    "        df.rename(columns={'Municipality': 'Municipality Code'}, inplace=True)\n",
    "    \n",
    "        df = df.set_index('Municipality Code')\n",
    "        df = df.T\n",
    "\n",
    "        df.columns.name = None\n",
    "        df.index.name = None\n",
    "        \n",
    "        df.columns = ['Cases']\n",
    "    \n",
    "    #df = df.reset_index()\n",
    "    #df.rename(columns={'index': 'epiweek'}, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39baa5ec",
   "metadata": {},
   "source": [
    "### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "677c638a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[        Cases\n",
       " 200701      1\n",
       " 200702      0\n",
       " 200703      0\n",
       " 200704      0\n",
       " 200705      0\n",
       " ...       ...\n",
       " 201948     15\n",
       " 201949     20\n",
       " 201950     30\n",
       " 201951     14\n",
       " 201952      5\n",
       " \n",
       " [676 rows x 1 columns],\n",
       "         Cases\n",
       " 200701      7\n",
       " 200702      0\n",
       " 200703      3\n",
       " 200704      4\n",
       " 200705      0\n",
       " ...       ...\n",
       " 201948    186\n",
       " 201949    212\n",
       " 201950    223\n",
       " 201951    242\n",
       " 201952    109\n",
       " \n",
       " [676 rows x 1 columns],\n",
       "         Cases\n",
       " 200701     10\n",
       " 200702      7\n",
       " 200703     21\n",
       " 200704     19\n",
       " 200705     16\n",
       " ...       ...\n",
       " 201948    103\n",
       " 201949     99\n",
       " 201950     66\n",
       " 201951     34\n",
       " 201952     25\n",
       " \n",
       " [676 rows x 1 columns],\n",
       "         Cases\n",
       " 200701     82\n",
       " 200702     63\n",
       " 200703     63\n",
       " 200704     70\n",
       " 200705    100\n",
       " ...       ...\n",
       " 201948     51\n",
       " 201949     43\n",
       " 201950     46\n",
       " 201951     35\n",
       " 201952     10\n",
       " \n",
       " [676 rows x 1 columns],\n",
       "         Cases\n",
       " 200701     16\n",
       " 200702     15\n",
       " 200703     13\n",
       " 200704     12\n",
       " 200705     17\n",
       " ...       ...\n",
       " 201948    149\n",
       " 201949    171\n",
       " 201950    175\n",
       " 201951    116\n",
       " 201952     43\n",
       " \n",
       " [676 rows x 1 columns]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df = [read_labels(path=labels, Municipality=municipality) for municipality in Municipalities]\n",
    "labels_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64c7bbf",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "690e7a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(images_dir):\n",
    "    \n",
    "    out_df = {\n",
    "        'epiweek':[],\n",
    "        'image':[]\n",
    "    }\n",
    "    \n",
    "    for image_path in os.listdir(images_dir):\n",
    "        if image_path.endswith('.tiff'):\n",
    "            epiweek = epiweek_from_date(image_path)\n",
    "            full_path = os.path.join(images_dir, image_path)\n",
    "            \n",
    "            out_df['epiweek'].append(epiweek)\n",
    "            out_df['image'].append(full_path)\n",
    "\n",
    "    df = pd.DataFrame(out_df)\n",
    "    \n",
    "    df = df.set_index('epiweek')\n",
    "    df.index.name = None\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9389e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>201731</th>\n",
       "      <td>DATASET_5_best_cities/Medellín/image_2017-07-3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201551</th>\n",
       "      <td>DATASET_5_best_cities/Medellín/image_2015-12-2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201747</th>\n",
       "      <td>DATASET_5_best_cities/Medellín/image_2017-11-1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201647</th>\n",
       "      <td>DATASET_5_best_cities/Medellín/image_2016-11-2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201729</th>\n",
       "      <td>DATASET_5_best_cities/Medellín/image_2017-07-1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    image\n",
       "201731  DATASET_5_best_cities/Medellín/image_2017-07-3...\n",
       "201551  DATASET_5_best_cities/Medellín/image_2015-12-2...\n",
       "201747  DATASET_5_best_cities/Medellín/image_2017-11-1...\n",
       "201647  DATASET_5_best_cities/Medellín/image_2016-11-2...\n",
       "201729  DATASET_5_best_cities/Medellín/image_2017-07-1..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if 'DATASET_5_best_cities' not in features:\n",
    "    for i in range(len(Municipalities)):\n",
    "        Municipalities[i] = codes[Municipalities[i]]\n",
    "    \n",
    "images_dir_list = [os.path.join(features, MUNICIPALITY) for MUNICIPALITY in Municipalities]\n",
    "\n",
    "features_df = [create_df(images_dir) for images_dir in images_dir_list]\n",
    "features_df[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b679a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(path, target_size):\n",
    "    # Read the image and convert to numpy array\n",
    "    image = io.imread(path)\n",
    "    # Resize the image and normalize values\n",
    "    image_arr = resize(image,(target_size[0], target_size[1]))\n",
    "    #print(f'The shape of the image before reshape: {image_arr.shape}, of type{type(image_arr)}')\n",
    "\n",
    "    # Select RGB bands\n",
    "    if target_size[2] == 3:\n",
    "        image_arr = image_arr[:,:, [1,2,3]]\n",
    "    return image_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccd6d9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(features_df)):\n",
    "    features_df[i].image = features_df[i].image.apply(read_image, args=[target_size])\n",
    "#features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de44a6ab",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17dd5c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = labels_df[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "896622e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two dataframes based on the date values\n",
    "dengue_df = [features_df[i].merge(labels_df[i], how='inner', left_index=True, right_index=True) for i in range(len(labels_df))]\n",
    "dengue_df = [dengue_df[i].sort_index() for i in range(len(dengue_df))]\n",
    "#dengue_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eced157d",
   "metadata": {},
   "source": [
    "### Train Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a54423fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, train_percentage = 80):\n",
    "    # We need a sequence so we can't split randomly\n",
    "    # To divide into Train and test we have to calculate the train percentage of the dataset:\n",
    "    size = df.shape[0]\n",
    "    split = int(size*(train_percentage/100))\n",
    "    \n",
    "    \"\"\" Train \"\"\"\n",
    "    # We will train with 1st percentage % of data and test with the rest\n",
    "    train_df = df.iloc[:split,:] ## percentage % train\n",
    "    \n",
    "    \"\"\" Test \"\"\"\n",
    "    test_df = df.iloc[split:,:] # 100 - percentage % test\n",
    "    \n",
    "    print(f'The train shape is: {train_df.shape}')\n",
    "    print(f'The test shape is: {test_df.shape}')\n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c07094c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train shape is: (132, 2)\n",
      "The test shape is: (33, 2)\n",
      "The train shape is: (132, 2)\n",
      "The test shape is: (33, 2)\n",
      "The train shape is: (132, 2)\n",
      "The test shape is: (33, 2)\n",
      "The train shape is: (132, 2)\n",
      "The test shape is: (33, 2)\n",
      "The train shape is: (132, 2)\n",
      "The test shape is: (33, 2)\n"
     ]
    }
   ],
   "source": [
    "train_df = []\n",
    "test_df = []\n",
    "\n",
    "for i in range(len(dengue_df)):\n",
    "    train_df_aux, test_df_aux = train_test_split(dengue_df[i], train_percentage = 80)\n",
    "    train_df.append(train_df_aux)\n",
    "    test_df.append(test_df_aux)\n",
    "#test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2376627e",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc6e6e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_train_labels(df, column, feature_range=(-1, 1)):\n",
    "    # Get values of the column\n",
    "    values = df[column].values.reshape(-1,1)\n",
    "    # Generate a new scaler\n",
    "    scaler = MinMaxScaler(feature_range=feature_range)\n",
    "    # Fit the scaler just for that column\n",
    "    scaled_column = scaler.fit_transform(values)\n",
    "    # Add the scaled column to the dataframe\n",
    "    scaled_column = np.reshape(scaled_column, len(scaled_column))\n",
    "    df[column] = scaled_column\n",
    "    return df, scaler\n",
    "    \n",
    "def normalize_test_labels(df, column, scaler):\n",
    "    # Get values of the column\n",
    "    values = df[column].values.reshape(-1,1)\n",
    "    # Scale values\n",
    "    scaled_column = scaler.transform(values)\n",
    "    scaled_column = np.reshape(scaled_column,len(scaled_column))\n",
    "    # Add the scaled values to the df\n",
    "    df[column] = scaled_column\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "770c0a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge:\n",
    "train_df = pd.concat([train_df[0], train_df[1], train_df[2], train_df[3], train_df[4]], keys=Municipalities)\n",
    "test_df = pd.concat([test_df[0], test_df[1], test_df[2], test_df[3], test_df[4]], keys=Municipalities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "108b64d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_range = (-1, 1)\n",
    "\n",
    "# Scale train:\n",
    "train_df, scaler = normalize_train_labels(train_df, 'Cases', feature_range=feature_range)\n",
    "train_df = [train_df[train_df.index.get_level_values(0) == municipality] for municipality in Municipalities]\n",
    "\n",
    "#train_df[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e93d56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_range = (-1, 1)\n",
    "\n",
    "# Scale test:\n",
    "test_df = normalize_test_labels(test_df, 'Cases', scaler=scaler)\n",
    "test_df = [test_df[test_df.index.get_level_values(0) == municipality] for municipality in Municipalities]\n",
    "\n",
    "#test_df[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990dddad",
   "metadata": {},
   "source": [
    "### Prepare data for time series supervised learning (function to create sliding window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "323b8a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for time series\n",
    "\n",
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True, no_autoregressive=None):\n",
    "    if no_autoregressive:\n",
    "        n_in = n_in - 1\n",
    "        \n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        if no_autoregressive:\n",
    "            cols.append(df.shift(i).iloc[:,:-1])\n",
    "            names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars-1)]\n",
    "        else:\n",
    "            cols.append(df.shift(i))\n",
    "            names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5739a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of window\n",
    "days = 10\n",
    "no_autoregressive = True\n",
    "\n",
    "# frame as supervised learning\n",
    "train = [series_to_supervised(df, n_in=days, no_autoregressive=no_autoregressive) for df in train_df]\n",
    "test = [series_to_supervised(df, n_in=days, no_autoregressive=no_autoregressive) for df in test_df]\n",
    "\n",
    "#DataFrame(train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72a4971",
   "metadata": {},
   "source": [
    "### Merge train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c20a873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge:\n",
    "train = pd.concat([train[0], train[1], train[2], train[3], train[4]], keys=Municipalities)\n",
    "test = pd.concat([test[0], test[1], test[2], test[3], test[4]], keys=Municipalities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9e7a366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_df_to_np(train):\n",
    "    for i, column in enumerate(train.columns):\n",
    "        if i == 0:\n",
    "            train_arr = np.array(train[column].to_list())\n",
    "            train_arr = np.expand_dims(train_arr, axis=1)\n",
    "\n",
    "        else:\n",
    "            #print(f'original: {train_arr.shape}')\n",
    "\n",
    "            train_arr_aux = np.array(train[column].to_list())\n",
    "            train_arr_aux = np.expand_dims(train_arr_aux, axis=1)\n",
    "\n",
    "            #print(f'aux: {train_arr_aux.shape}')\n",
    "\n",
    "            train_arr = np.concatenate((train_arr, train_arr_aux), axis=1)\n",
    "\n",
    "    return train_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c82676",
   "metadata": {},
   "source": [
    "### Features and Labels Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "229a1b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_labels_set(timeseries_data, original_df):\n",
    "    \n",
    "    \"\"\" Features \"\"\"\n",
    "    # We define the number of features as (Cases and media cloud)\n",
    "    n_features = original_df.shape[1]\n",
    "\n",
    "    # The features to train the model will be all except the values of the actual week \n",
    "    # We can't use other variables in week t because whe need to resample a a 3D Array\n",
    "    features_set = DataFrame(timeseries_data.values[:,:-1])\n",
    "    # Convert pandas data frame to np.array to reshape as 3D Array\n",
    "    features_set = convert_df_to_np(features_set)\n",
    "    print(f'The shape of the features is {features_set.shape}')\n",
    "    \n",
    "    \"\"\" Labels \"\"\"\n",
    "    # We will use Covid cases in last week \n",
    "    labels_set = DataFrame(timeseries_data.values[:,-1])\n",
    "    # Convert pandas data frame to np.array\n",
    "    labels_set = labels_set.to_numpy()\n",
    "    print(f'The shape of the labels is {labels_set.shape}')\n",
    "    \n",
    "    return features_set, labels_set, n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d9537ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "The shape of the features is (615, 10, 224, 224, 12)\n",
      "The shape of the labels is (615, 1)\n",
      "Test:\n",
      "The shape of the features is (120, 10, 224, 224, 12)\n",
      "The shape of the labels is (120, 1)\n"
     ]
    }
   ],
   "source": [
    "# Train features and labels set\n",
    "print('Train:')\n",
    "train_X, train_y, n_features = features_labels_set(timeseries_data=train, original_df=dengue_df[0])\n",
    "\n",
    "# Test features and labels set\n",
    "print('Test:')\n",
    "test_X, test_y, n_features = features_labels_set(timeseries_data=test, original_df=dengue_df[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73b586c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.asarray(train_X).astype(np.float32)\n",
    "train_y = np.asarray(train_y).astype(np.float32)\n",
    "\n",
    "test_X = np.asarray(test_X).astype(np.float32)\n",
    "test_y = np.asarray(test_y).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9537a697",
   "metadata": {},
   "source": [
    "# Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3255361b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seed\n",
    "#tf.random.set_seed(0)\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    epsilon = 0.1\n",
    "    summ = K.maximum(K.abs(y_true) + K.abs(y_pred) + epsilon, 0.5 + epsilon)\n",
    "    smape = K.abs(y_pred - y_true) / summ * 2.0\n",
    "    return smape\n",
    "\n",
    "\n",
    "def create_model(backbone=backbone):\n",
    "    lstm_week, input_shape = days, target_size\n",
    "    \n",
    "    # design network\n",
    "    model = Sequential()\n",
    "\n",
    "    # CNN\n",
    "    cnn = get_Variational_Autoencoder(model_path=backbone, backbone=True)\n",
    "\n",
    "    for idx, layer in enumerate(cnn.layers):\n",
    "        layer.trainable = False # idx > len(cnn.layers) - 2 \n",
    "    \n",
    "    # https://levelup.gitconnected.com/hands-on-practice-with-time-distributed-layers-using-tensorflow-c776a5d78e7e\n",
    "    model.add(keras.layers.TimeDistributed(cnn, input_shape = ((lstm_week,) + input_shape)))\n",
    "    model.add(keras.layers.TimeDistributed(Flatten()))\n",
    "    model.add(keras.layers.TimeDistributed(Dense(1024)))\n",
    "    model.add(LSTM(120, dropout=0.1, return_sequences=True))\n",
    "    model.add(LSTM(240, dropout=0.1, return_sequences = False))\n",
    "    model.add(Dense(60))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Compile the model:\n",
    "    opt = keras.optimizers.Adam()\n",
    "    \n",
    "    # Metrics\n",
    "    metrics = [\n",
    "        tf.keras.metrics.RootMeanSquaredError(name='rmse'),\n",
    "        tf.keras.metrics.MeanAbsolutePercentageError(name='mape'),\n",
    "        smape\n",
    "    ]\n",
    "    \n",
    "    model.compile(loss='mse', optimizer=opt, metrics=metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8c3537",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a3b253f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# EarlyStopping:\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=20, \n",
    "        verbose=1, mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef9d6117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit network\n",
    "def train_model(model, monitor, plot=None, epochs=20):\n",
    "    if monitor:\n",
    "        history = model.fit(train_X, train_y, epochs=epochs, batch_size=16, validation_data=(test_X, test_y), verbose=2, shuffle=False, callbacks=[monitor])\n",
    "    else:\n",
    "        history = model.fit(train_X, train_y, epochs=epochs, batch_size=16, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "    \n",
    "    if plot:\n",
    "        # plot history\n",
    "        plt.plot(history.history['loss'], label='train')\n",
    "        plt.plot(history.history['val_loss'], label='validation')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcd330c",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "18f364c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "\n",
    "def test_model(model, test_X, test_y, scaler, rnn = None):\n",
    "    \n",
    "    # If model is a classical machine learning model and test_X is a 3D tensor, then convert to 2D\n",
    "    if not rnn and (len(test_X.shape) == 3):\n",
    "        test_X = test_X.reshape((test_X.shape[0], -1))\n",
    "    \n",
    "    # do the prediction\n",
    "    yhat = model.predict(test_X)\n",
    "    \n",
    "    # Invert scaling for forecast\n",
    "    # Inverse Scaler\n",
    "    \n",
    "    # Predicted\n",
    "    if not rnn:\n",
    "        yhat = yhat.reshape(-1, 1)\n",
    "        \n",
    "    if not scaler:\n",
    "        return yhat, test_y\n",
    "    \n",
    "    inv_yhat = scaler.inverse_transform(yhat)\n",
    "    \n",
    "    # Real:\n",
    "    inv_y = scaler.inverse_transform(test_y)\n",
    "    \n",
    "    return inv_yhat, inv_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35771b29",
   "metadata": {},
   "source": [
    "### Mean Absolute Percentage Error (MAPE)\n",
    "\n",
    "$$\n",
    "MAPE = \\displaystyle\\frac{100\\%}{n}\\sum_{t=1}^{n}\\left |\\frac{x_i-y_i}{y_t}\\right|\n",
    "$$\n",
    "\n",
    "MAPE has a problem if there are zeros in the test data, so other metrics can be explored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d3b634a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    print('Test MAPE: %.3f' % mape)\n",
    "    return mape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d67192",
   "metadata": {},
   "source": [
    "### Symmetric Mean Absolute Percentage Error (sMAPE)\n",
    "\n",
    "$$\n",
    "sMAPE = \\displaystyle\\frac{100\\%}{n}\\sum_{t=1}^{n} \\frac{|x_i-y_i|}{|x_i|+|y_t|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7fff1fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def symmetric_mean_absolute_percentage_error(y_true, y_pred):\n",
    "    \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    smape = 1/len(y_true) * np.sum(2 * np.abs(y_pred-y_true) / (np.abs(y_true) + np.abs(y_pred))*100)\n",
    "    print('Test sMAPE: %.3f' % smape)\n",
    "    return smape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c386a8",
   "metadata": {},
   "source": [
    "### Root Mean Squared Error (RMSE)\n",
    "$$\n",
    "RMSE = \\sqrt{(\\frac{1}{n})\\sum_{i=1}^{n}(x_i-y_i)^{2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7586fd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    print('Test RMSE: %.3f' % rmse)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1fe25de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(inv_y, inv_yhat, model_name = ''):\n",
    "    data_predict = inv_yhat  ## predicted target cases\n",
    "    dataY_plot = inv_y  ##  real test-target cases\n",
    "\n",
    "    data_predict = data_predict.reshape(len(data_predict), 1)\n",
    "    dataY_plot = dataY_plot.reshape(len(dataY_plot), 1)\n",
    "\n",
    "    plt.plot(dataY_plot, label = 'actual')\n",
    "    plt.plot(data_predict, label = 'predicted')\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.suptitle(f'Time-Series Prediction with {model_name}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f8ee315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_X, test_y, scaler):\n",
    "    stored_results = {}\n",
    "    inv_yhat_lstm, inv_y_lstm = test_model(model=model, test_X=test_X, test_y=test_y, scaler=scaler, rnn = True)\n",
    "    stored_results['mape'] = mean_absolute_percentage_error(inv_y_lstm, inv_yhat_lstm)\n",
    "    stored_results['smape'] = symmetric_mean_absolute_percentage_error(inv_y_lstm, inv_yhat_lstm)\n",
    "    stored_results['rmse'] = root_mean_squared_error(inv_y_lstm, inv_yhat_lstm)\n",
    "\n",
    "    return stored_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "509eecd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmodel = create_model(backbone=backbone)\\ntrain_model(model=model, monitor=monitor)\\n\\nmodel.save(f'Models/{backbone}_LSTM_Regression.h5')\\nmodel.summary()\\n\\ninv_yhat_lstm, inv_y_lstm = test_model(model=model, test_X=test_X, test_y=test_y, scaler=scaler, rnn = True)\\n\\nevaluate(model, test_X, test_y, scaler)\\n\\n# LSTM\\nplot_predictions(inv_y_lstm, inv_yhat_lstm, model_name = 'LSTM')\\n\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "model = create_model(backbone=backbone)\n",
    "train_model(model=model, monitor=monitor)\n",
    "\n",
    "model.save(f'Models/{backbone}_LSTM_Regression.h5')\n",
    "model.summary()\n",
    "\n",
    "inv_yhat_lstm, inv_y_lstm = test_model(model=model, test_X=test_X, test_y=test_y, scaler=scaler, rnn = True)\n",
    "\n",
    "evaluate(model, test_X, test_y, scaler)\n",
    "\n",
    "# LSTM\n",
    "plot_predictions(inv_y_lstm, inv_yhat_lstm, model_name = 'LSTM')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d351813c",
   "metadata": {},
   "source": [
    "# Calculate Mean and SD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f863c3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With LSTM:\n",
    "#print(f'The scalers are: {scalers.keys()}')\n",
    "#y_scaler = scalers['scaler_Cases']\n",
    "\n",
    "def calculate_mean_std():\n",
    "    \n",
    "    metrics = {\n",
    "        \"rmse\": [],\n",
    "        \"mape\": [],\n",
    "        \"smape\": []\n",
    "    }\n",
    "    \n",
    "    for i in range(5):\n",
    "        model = create_model(backbone=backbone)\n",
    "        train_model(model=model, monitor=monitor)\n",
    "        stored_results = evaluate(model, test_X, test_y, scaler)\n",
    "        print(stored_results)\n",
    "        \n",
    "        for key in metrics.keys():\n",
    "            metrics[key].append(stored_results[key])\n",
    "            \n",
    "    for key in metrics.keys():\n",
    "        results = metrics[key]\n",
    "        print(key, f\": average={np.average(results):.3f}, std={np.std(results):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43c3485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/datascience/conda/generalml_p37_gpu_v1/lib/python3.7/site-packages/keras/layers/normalization/batch_normalization.py:520: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Train on 615 samples, validate on 120 samples\n",
      "Epoch 1/20\n",
      "615/615 - 18s - loss: 0.2555 - rmse: 0.5055 - mape: 81.3880 - smape: 0.5754 - val_loss: 0.0450 - val_rmse: 0.2121 - val_mape: 19.2587 - val_smape: 0.1713\n",
      "Epoch 2/20\n",
      "615/615 - 17s - loss: 0.1586 - rmse: 0.3982 - mape: 92.4030 - smape: 0.4550 - val_loss: 0.0204 - val_rmse: 0.1430 - val_mape: 12.8308 - val_smape: 0.1184\n",
      "Epoch 3/20\n",
      "615/615 - 17s - loss: 0.1618 - rmse: 0.4023 - mape: 102.4439 - smape: 0.4240 - val_loss: 0.0195 - val_rmse: 0.1397 - val_mape: 12.6220 - val_smape: 0.1227\n",
      "Epoch 4/20\n",
      "615/615 - 17s - loss: 0.1478 - rmse: 0.3845 - mape: 94.7670 - smape: 0.4048 - val_loss: 0.0219 - val_rmse: 0.1480 - val_mape: 13.9177 - val_smape: 0.1395\n",
      "Epoch 5/20\n",
      "615/615 - 17s - loss: 0.1394 - rmse: 0.3733 - mape: 91.9032 - smape: 0.3862 - val_loss: 0.0169 - val_rmse: 0.1300 - val_mape: 12.3871 - val_smape: 0.1187\n",
      "Epoch 6/20\n",
      "615/615 - 17s - loss: 0.1453 - rmse: 0.3812 - mape: 95.2659 - smape: 0.4017 - val_loss: 0.0134 - val_rmse: 0.1156 - val_mape: 10.4511 - val_smape: 0.0993\n",
      "Epoch 7/20\n",
      "615/615 - 17s - loss: 0.1394 - rmse: 0.3734 - mape: 94.9810 - smape: 0.3930 - val_loss: 0.0125 - val_rmse: 0.1119 - val_mape: 9.9869 - val_smape: 0.0900\n",
      "Epoch 8/20\n",
      "615/615 - 17s - loss: 0.1458 - rmse: 0.3818 - mape: 94.3834 - smape: 0.4000 - val_loss: 0.0132 - val_rmse: 0.1147 - val_mape: 9.8058 - val_smape: 0.0840\n",
      "Epoch 9/20\n",
      "615/615 - 17s - loss: 0.1551 - rmse: 0.3938 - mape: 99.5832 - smape: 0.4080 - val_loss: 0.0151 - val_rmse: 0.1227 - val_mape: 10.5188 - val_smape: 0.0883\n",
      "Epoch 10/20\n",
      "615/615 - 17s - loss: 0.1571 - rmse: 0.3963 - mape: 102.8559 - smape: 0.4039 - val_loss: 0.0213 - val_rmse: 0.1458 - val_mape: 13.3579 - val_smape: 0.1106\n",
      "Epoch 11/20\n",
      "615/615 - 17s - loss: 0.1609 - rmse: 0.4011 - mape: 105.2162 - smape: 0.4010 - val_loss: 0.0226 - val_rmse: 0.1504 - val_mape: 13.9529 - val_smape: 0.1152\n",
      "Epoch 12/20\n",
      "615/615 - 17s - loss: 0.1622 - rmse: 0.4027 - mape: 106.7702 - smape: 0.3986 - val_loss: 0.0334 - val_rmse: 0.1829 - val_mape: 18.4319 - val_smape: 0.1520\n",
      "Epoch 13/20\n",
      "615/615 - 17s - loss: 0.1669 - rmse: 0.4085 - mape: 109.0435 - smape: 0.3861 - val_loss: 0.0287 - val_rmse: 0.1693 - val_mape: 16.6000 - val_smape: 0.1370\n",
      "Epoch 14/20\n",
      "615/615 - 17s - loss: 0.1682 - rmse: 0.4101 - mape: 110.0131 - smape: 0.3733 - val_loss: 0.0230 - val_rmse: 0.1517 - val_mape: 13.9663 - val_smape: 0.1150\n",
      "Epoch 15/20\n",
      "615/615 - 17s - loss: 0.1651 - rmse: 0.4064 - mape: 111.8040 - smape: 0.3662 - val_loss: 0.0136 - val_rmse: 0.1166 - val_mape: 9.6527 - val_smape: 0.0808\n",
      "Epoch 16/20\n",
      "615/615 - 17s - loss: 0.1584 - rmse: 0.3980 - mape: 111.0266 - smape: 0.3567 - val_loss: 0.0110 - val_rmse: 0.1050 - val_mape: 8.8273 - val_smape: 0.0756\n",
      "Epoch 17/20\n",
      "615/615 - 17s - loss: 0.1538 - rmse: 0.3922 - mape: 107.6425 - smape: 0.3451 - val_loss: 0.0091 - val_rmse: 0.0953 - val_mape: 7.6657 - val_smape: 0.0656\n",
      "Epoch 18/20\n",
      "615/615 - 17s - loss: 0.1510 - rmse: 0.3886 - mape: 107.7626 - smape: 0.3401 - val_loss: 0.0105 - val_rmse: 0.1025 - val_mape: 8.8131 - val_smape: 0.0769\n",
      "Epoch 19/20\n",
      "615/615 - 17s - loss: 0.1485 - rmse: 0.3853 - mape: 107.8829 - smape: 0.3378 - val_loss: 0.0096 - val_rmse: 0.0978 - val_mape: 8.5045 - val_smape: 0.0749\n",
      "Epoch 20/20\n",
      "615/615 - 17s - loss: 0.1443 - rmse: 0.3799 - mape: 106.5284 - smape: 0.3317 - val_loss: 0.0101 - val_rmse: 0.1007 - val_mape: 8.8298 - val_smape: 0.0786\n",
      "Test MAPE: 119.614\n",
      "Test sMAPE: 71.691\n",
      "Test RMSE: 44.018\n",
      "{'mape': 119.61407661437988, 'smape': 71.6911376953125, 'rmse': 44.017757}\n",
      "Train on 615 samples, validate on 120 samples\n",
      "Epoch 1/20\n",
      "615/615 - 18s - loss: 0.2255 - rmse: 0.4749 - mape: 82.8400 - smape: 0.5623 - val_loss: 0.0340 - val_rmse: 0.1845 - val_mape: 17.4586 - val_smape: 0.1553\n",
      "Epoch 2/20\n",
      "615/615 - 17s - loss: 0.1669 - rmse: 0.4085 - mape: 102.4839 - smape: 0.4585 - val_loss: 0.0255 - val_rmse: 0.1596 - val_mape: 14.7963 - val_smape: 0.1309\n",
      "Epoch 3/20\n",
      "615/615 - 17s - loss: 0.1614 - rmse: 0.4018 - mape: 99.3462 - smape: 0.4315 - val_loss: 0.0229 - val_rmse: 0.1513 - val_mape: 13.9420 - val_smape: 0.1362\n",
      "Epoch 4/20\n",
      "615/615 - 17s - loss: 0.1532 - rmse: 0.3914 - mape: 95.3792 - smape: 0.4091 - val_loss: 0.0162 - val_rmse: 0.1273 - val_mape: 11.7176 - val_smape: 0.1128\n",
      "Epoch 5/20\n",
      "615/615 - 17s - loss: 0.1497 - rmse: 0.3869 - mape: 94.8315 - smape: 0.4123 - val_loss: 0.0150 - val_rmse: 0.1226 - val_mape: 11.9802 - val_smape: 0.1137\n",
      "Epoch 6/20\n",
      "615/615 - 17s - loss: 0.1492 - rmse: 0.3863 - mape: 94.4901 - smape: 0.4088 - val_loss: 0.0135 - val_rmse: 0.1161 - val_mape: 10.6081 - val_smape: 0.0995\n",
      "Epoch 7/20\n",
      "615/615 - 17s - loss: 0.1509 - rmse: 0.3884 - mape: 95.5831 - smape: 0.4039 - val_loss: 0.0156 - val_rmse: 0.1247 - val_mape: 11.1176 - val_smape: 0.0956\n",
      "Epoch 8/20\n",
      "615/615 - 17s - loss: 0.1521 - rmse: 0.3900 - mape: 99.7392 - smape: 0.3996 - val_loss: 0.0143 - val_rmse: 0.1197 - val_mape: 10.6030 - val_smape: 0.0928\n",
      "Epoch 9/20\n",
      "615/615 - 17s - loss: 0.1507 - rmse: 0.3881 - mape: 96.2992 - smape: 0.3994 - val_loss: 0.0180 - val_rmse: 0.1343 - val_mape: 12.1387 - val_smape: 0.1026\n",
      "Epoch 10/20\n",
      "615/615 - 17s - loss: 0.1559 - rmse: 0.3949 - mape: 102.5205 - smape: 0.3965 - val_loss: 0.0158 - val_rmse: 0.1256 - val_mape: 11.3599 - val_smape: 0.0966\n",
      "Epoch 11/20\n",
      "615/615 - 17s - loss: 0.1575 - rmse: 0.3969 - mape: 100.9017 - smape: 0.4044 - val_loss: 0.0288 - val_rmse: 0.1698 - val_mape: 16.3668 - val_smape: 0.1352\n",
      "Epoch 12/20\n",
      "615/615 - 17s - loss: 0.1636 - rmse: 0.4044 - mape: 106.9960 - smape: 0.3830 - val_loss: 0.0326 - val_rmse: 0.1806 - val_mape: 17.6621 - val_smape: 0.1460\n",
      "Epoch 13/20\n",
      "615/615 - 17s - loss: 0.1661 - rmse: 0.4075 - mape: 110.4063 - smape: 0.3859 - val_loss: 0.0303 - val_rmse: 0.1740 - val_mape: 16.9319 - val_smape: 0.1398\n",
      "Epoch 14/20\n",
      "615/615 - 17s - loss: 0.1631 - rmse: 0.4038 - mape: 106.6106 - smape: 0.3773 - val_loss: 0.0276 - val_rmse: 0.1662 - val_mape: 16.0343 - val_smape: 0.1332\n",
      "Epoch 15/20\n",
      "615/615 - 17s - loss: 0.1629 - rmse: 0.4036 - mape: 112.4684 - smape: 0.3697 - val_loss: 0.0195 - val_rmse: 0.1395 - val_mape: 12.9973 - val_smape: 0.1092\n",
      "Epoch 16/20\n",
      "615/615 - 17s - loss: 0.1634 - rmse: 0.4042 - mape: 108.4515 - smape: 0.3671 - val_loss: 0.0158 - val_rmse: 0.1257 - val_mape: 11.0009 - val_smape: 0.0924\n",
      "Epoch 17/20\n",
      "615/615 - 17s - loss: 0.1575 - rmse: 0.3969 - mape: 110.1238 - smape: 0.3587 - val_loss: 0.0117 - val_rmse: 0.1081 - val_mape: 9.1983 - val_smape: 0.0782\n",
      "Epoch 18/20\n",
      "615/615 - 17s - loss: 0.1523 - rmse: 0.3903 - mape: 106.2870 - smape: 0.3428 - val_loss: 0.0103 - val_rmse: 0.1015 - val_mape: 8.3631 - val_smape: 0.0717\n",
      "Epoch 19/20\n",
      "615/615 - 17s - loss: 0.1488 - rmse: 0.3858 - mape: 105.0729 - smape: 0.3402 - val_loss: 0.0107 - val_rmse: 0.1033 - val_mape: 8.5831 - val_smape: 0.0739\n",
      "Epoch 20/20\n",
      "615/615 - 17s - loss: 0.1465 - rmse: 0.3827 - mape: 105.9372 - smape: 0.3369 - val_loss: 0.0103 - val_rmse: 0.1016 - val_mape: 8.8871 - val_smape: 0.0781\n",
      "Test MAPE: 120.677\n",
      "Test sMAPE: 67.650\n",
      "Test RMSE: 41.434\n",
      "{'mape': 120.67698240280151, 'smape': 67.64966634114583, 'rmse': 41.43372}\n",
      "Train on 615 samples, validate on 120 samples\n",
      "Epoch 1/20\n",
      "615/615 - 18s - loss: 0.2294 - rmse: 0.4789 - mape: 95.6416 - smape: 0.5659 - val_loss: 0.0631 - val_rmse: 0.2512 - val_mape: 23.0341 - val_smape: 0.1932\n",
      "Epoch 2/20\n",
      "615/615 - 17s - loss: 0.1989 - rmse: 0.4459 - mape: 106.8725 - smape: 0.4857 - val_loss: 0.0276 - val_rmse: 0.1661 - val_mape: 15.1911 - val_smape: 0.1417\n",
      "Epoch 3/20\n",
      "615/615 - 17s - loss: 0.1769 - rmse: 0.4207 - mape: 102.7088 - smape: 0.4466 - val_loss: 0.0230 - val_rmse: 0.1516 - val_mape: 14.4126 - val_smape: 0.1420\n",
      "Epoch 4/20\n",
      "615/615 - 17s - loss: 0.1469 - rmse: 0.3832 - mape: 93.9196 - smape: 0.4094 - val_loss: 0.0184 - val_rmse: 0.1355 - val_mape: 12.5350 - val_smape: 0.1199\n",
      "Epoch 5/20\n",
      "615/615 - 17s - loss: 0.1437 - rmse: 0.3790 - mape: 97.8899 - smape: 0.3959 - val_loss: 0.0179 - val_rmse: 0.1338 - val_mape: 11.9411 - val_smape: 0.1168\n",
      "Epoch 6/20\n",
      "615/615 - 17s - loss: 0.1492 - rmse: 0.3863 - mape: 96.3186 - smape: 0.4044 - val_loss: 0.0174 - val_rmse: 0.1321 - val_mape: 11.7477 - val_smape: 0.1050\n",
      "Epoch 7/20\n",
      "615/615 - 17s - loss: 0.1528 - rmse: 0.3908 - mape: 97.7352 - smape: 0.4102 - val_loss: 0.0140 - val_rmse: 0.1181 - val_mape: 10.5125 - val_smape: 0.0941\n",
      "Epoch 8/20\n",
      "615/615 - 17s - loss: 0.1520 - rmse: 0.3898 - mape: 98.4635 - smape: 0.4062 - val_loss: 0.0127 - val_rmse: 0.1125 - val_mape: 10.0581 - val_smape: 0.0891\n",
      "Epoch 9/20\n",
      "615/615 - 17s - loss: 0.1567 - rmse: 0.3958 - mape: 98.3335 - smape: 0.4125 - val_loss: 0.0193 - val_rmse: 0.1388 - val_mape: 12.7363 - val_smape: 0.1085\n",
      "Epoch 10/20\n",
      "615/615 - 17s - loss: 0.1602 - rmse: 0.4002 - mape: 104.0692 - smape: 0.4046 - val_loss: 0.0301 - val_rmse: 0.1735 - val_mape: 15.8572 - val_smape: 0.1296\n",
      "Epoch 11/20\n",
      "615/615 - 17s - loss: 0.1654 - rmse: 0.4067 - mape: 106.1313 - smape: 0.4047 - val_loss: 0.0333 - val_rmse: 0.1825 - val_mape: 17.8943 - val_smape: 0.1478\n",
      "Epoch 12/20\n",
      "615/615 - 17s - loss: 0.1683 - rmse: 0.4102 - mape: 109.2016 - smape: 0.3962 - val_loss: 0.0344 - val_rmse: 0.1855 - val_mape: 17.9657 - val_smape: 0.1474\n",
      "Epoch 13/20\n",
      "615/615 - 17s - loss: 0.1642 - rmse: 0.4052 - mape: 108.9889 - smape: 0.3804 - val_loss: 0.0299 - val_rmse: 0.1728 - val_mape: 16.9197 - val_smape: 0.1400\n",
      "Epoch 14/20\n",
      "615/615 - 17s - loss: 0.1657 - rmse: 0.4071 - mape: 112.0625 - smape: 0.3763 - val_loss: 0.0253 - val_rmse: 0.1591 - val_mape: 14.6880 - val_smape: 0.1213\n",
      "Epoch 15/20\n",
      "615/615 - 17s - loss: 0.1603 - rmse: 0.4004 - mape: 112.6299 - smape: 0.3597 - val_loss: 0.0177 - val_rmse: 0.1330 - val_mape: 11.9910 - val_smape: 0.1008\n",
      "Epoch 16/20\n",
      "615/615 - 17s - loss: 0.1589 - rmse: 0.3986 - mape: 110.7860 - smape: 0.3604 - val_loss: 0.0136 - val_rmse: 0.1164 - val_mape: 9.9088 - val_smape: 0.0838\n",
      "Epoch 17/20\n",
      "615/615 - 17s - loss: 0.1559 - rmse: 0.3948 - mape: 110.7262 - smape: 0.3491 - val_loss: 0.0124 - val_rmse: 0.1114 - val_mape: 9.6194 - val_smape: 0.0824\n",
      "Epoch 18/20\n",
      "615/615 - 17s - loss: 0.1515 - rmse: 0.3893 - mape: 107.2251 - smape: 0.3428 - val_loss: 0.0118 - val_rmse: 0.1087 - val_mape: 9.7639 - val_smape: 0.0859\n",
      "Epoch 19/20\n",
      "615/615 - 17s - loss: 0.1509 - rmse: 0.3885 - mape: 108.6208 - smape: 0.3428 - val_loss: 0.0098 - val_rmse: 0.0992 - val_mape: 8.6811 - val_smape: 0.0761\n",
      "Epoch 20/20\n",
      "615/615 - 17s - loss: 0.1474 - rmse: 0.3840 - mape: 106.8341 - smape: 0.3340 - val_loss: 0.0110 - val_rmse: 0.1047 - val_mape: 8.7687 - val_smape: 0.0762\n",
      "Test MAPE: 104.889\n",
      "Test sMAPE: 78.371\n",
      "Test RMSE: 46.257\n",
      "{'mape': 104.8892617225647, 'smape': 78.37057291666666, 'rmse': 46.257164}\n",
      "Train on 615 samples, validate on 120 samples\n",
      "Epoch 1/20\n",
      "615/615 - 19s - loss: 0.2242 - rmse: 0.4734 - mape: 73.0932 - smape: 0.5488 - val_loss: 0.0405 - val_rmse: 0.2014 - val_mape: 18.4284 - val_smape: 0.1618\n",
      "Epoch 2/20\n",
      "615/615 - 17s - loss: 0.1624 - rmse: 0.4030 - mape: 96.7138 - smape: 0.4504 - val_loss: 0.0259 - val_rmse: 0.1610 - val_mape: 15.1388 - val_smape: 0.1343\n",
      "Epoch 3/20\n",
      "615/615 - 17s - loss: 0.1652 - rmse: 0.4064 - mape: 99.8897 - smape: 0.4360 - val_loss: 0.0205 - val_rmse: 0.1430 - val_mape: 13.1221 - val_smape: 0.1299\n",
      "Epoch 4/20\n",
      "615/615 - 17s - loss: 0.1461 - rmse: 0.3822 - mape: 94.7281 - smape: 0.4078 - val_loss: 0.0160 - val_rmse: 0.1264 - val_mape: 12.0222 - val_smape: 0.1150\n",
      "Epoch 5/20\n",
      "615/615 - 17s - loss: 0.1502 - rmse: 0.3876 - mape: 97.3463 - smape: 0.4051 - val_loss: 0.0143 - val_rmse: 0.1198 - val_mape: 10.5875 - val_smape: 0.0944\n",
      "Epoch 6/20\n",
      "615/615 - 17s - loss: 0.1513 - rmse: 0.3890 - mape: 96.8809 - smape: 0.4075 - val_loss: 0.0188 - val_rmse: 0.1370 - val_mape: 12.4739 - val_smape: 0.1176\n",
      "Epoch 7/20\n",
      "615/615 - 17s - loss: 0.1487 - rmse: 0.3856 - mape: 96.9018 - smape: 0.4031 - val_loss: 0.0113 - val_rmse: 0.1062 - val_mape: 9.4190 - val_smape: 0.0831\n",
      "Epoch 8/20\n",
      "615/615 - 17s - loss: 0.1538 - rmse: 0.3921 - mape: 97.8284 - smape: 0.4070 - val_loss: 0.0176 - val_rmse: 0.1327 - val_mape: 11.8107 - val_smape: 0.0991\n",
      "Epoch 9/20\n",
      "615/615 - 17s - loss: 0.1614 - rmse: 0.4018 - mape: 101.8174 - smape: 0.4138 - val_loss: 0.0177 - val_rmse: 0.1331 - val_mape: 11.5352 - val_smape: 0.0964\n",
      "Epoch 10/20\n",
      "615/615 - 17s - loss: 0.1604 - rmse: 0.4005 - mape: 99.1880 - smape: 0.4139 - val_loss: 0.0278 - val_rmse: 0.1666 - val_mape: 16.3561 - val_smape: 0.1358\n",
      "Epoch 11/20\n",
      "615/615 - 17s - loss: 0.1642 - rmse: 0.4052 - mape: 106.7152 - smape: 0.3928 - val_loss: 0.0348 - val_rmse: 0.1867 - val_mape: 18.2348 - val_smape: 0.1494\n",
      "Epoch 12/20\n",
      "615/615 - 17s - loss: 0.1757 - rmse: 0.4192 - mape: 110.6768 - smape: 0.4025 - val_loss: 0.0359 - val_rmse: 0.1894 - val_mape: 19.0679 - val_smape: 0.1572\n",
      "Epoch 13/20\n",
      "615/615 - 17s - loss: 0.1666 - rmse: 0.4082 - mape: 111.3953 - smape: 0.3765 - val_loss: 0.0306 - val_rmse: 0.1750 - val_mape: 16.7572 - val_smape: 0.1373\n",
      "Epoch 14/20\n",
      "615/615 - 17s - loss: 0.1677 - rmse: 0.4095 - mape: 112.3276 - smape: 0.3699 - val_loss: 0.0320 - val_rmse: 0.1789 - val_mape: 17.1665 - val_smape: 0.1406\n",
      "Epoch 15/20\n",
      "615/615 - 17s - loss: 0.1657 - rmse: 0.4071 - mape: 110.4858 - smape: 0.3722 - val_loss: 0.0166 - val_rmse: 0.1288 - val_mape: 11.3500 - val_smape: 0.0946\n",
      "Epoch 16/20\n",
      "615/615 - 17s - loss: 0.1608 - rmse: 0.4010 - mape: 111.1691 - smape: 0.3622 - val_loss: 0.0115 - val_rmse: 0.1073 - val_mape: 8.7794 - val_smape: 0.0739\n",
      "Epoch 17/20\n",
      "615/615 - 17s - loss: 0.1524 - rmse: 0.3904 - mape: 106.9859 - smape: 0.3443 - val_loss: 0.0122 - val_rmse: 0.1103 - val_mape: 9.2938 - val_smape: 0.0792\n",
      "Epoch 18/20\n",
      "615/615 - 17s - loss: 0.1530 - rmse: 0.3912 - mape: 108.9716 - smape: 0.3454 - val_loss: 0.0099 - val_rmse: 0.0993 - val_mape: 8.6070 - val_smape: 0.0749\n",
      "Epoch 19/20\n",
      "615/615 - 17s - loss: 0.1509 - rmse: 0.3885 - mape: 107.2583 - smape: 0.3430 - val_loss: 0.0101 - val_rmse: 0.1005 - val_mape: 8.8830 - val_smape: 0.0783\n",
      "Epoch 20/20\n",
      "615/615 - 17s - loss: 0.1457 - rmse: 0.3817 - mape: 107.9779 - smape: 0.3350 - val_loss: 0.0085 - val_rmse: 0.0922 - val_mape: 8.0958 - val_smape: 0.0722\n",
      "Test MAPE: 126.752\n",
      "Test sMAPE: 73.219\n",
      "Test RMSE: 44.872\n",
      "{'mape': 126.75206661224365, 'smape': 73.21888020833333, 'rmse': 44.8717}\n",
      "Train on 615 samples, validate on 120 samples\n",
      "Epoch 1/20\n",
      "615/615 - 19s - loss: 0.2424 - rmse: 0.4924 - mape: 88.0755 - smape: 0.5736 - val_loss: 0.0675 - val_rmse: 0.2598 - val_mape: 24.6439 - val_smape: 0.1967\n",
      "Epoch 2/20\n",
      "615/615 - 17s - loss: 0.1848 - rmse: 0.4299 - mape: 96.2757 - smape: 0.4790 - val_loss: 0.0325 - val_rmse: 0.1802 - val_mape: 16.9241 - val_smape: 0.1424\n",
      "Epoch 3/20\n"
     ]
    }
   ],
   "source": [
    "calculate_mean_std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7638287",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:generalml_p37_gpu_v1]",
   "language": "python",
   "name": "conda-env-generalml_p37_gpu_v1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
